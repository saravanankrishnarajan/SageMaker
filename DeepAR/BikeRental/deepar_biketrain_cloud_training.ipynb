{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4efe93af",
   "metadata": {},
   "source": [
    "### DeepAR Model - Bike Rental Training\n",
    "\n",
    "Note: This data is not a true timeseries as there are lots of gaps\n",
    "\n",
    "We have data only for first 20 days of each month and model needs to predict the rental for the remaining days of the month. The dataset consists of two years data. DeepAR will shine with true multiple-timeseries dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0e36b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a1129db",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import s3 bucket name as environment variable\n",
    "\n",
    "import os\n",
    "env_vars = !cat ./.env\n",
    "for var in env_vars:\n",
    "    key, value = var.split('=')\n",
    "    os.environ[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b17195b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set differnt job names when building different models based on choices\n",
    "# Also have jobnames easily differiantiate according to choice\n",
    "\n",
    "with_categories = False\n",
    "if with_categories:\n",
    "    base_job_name = 'deepar-biketrain-with-categories'\n",
    "else:\n",
    "    base_job_name = 'deepar-biketrain-no-categories'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3635d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify your bucket name and dataset path in that\n",
    "\n",
    "bucket = os.environ['BUCKET_NAME']\n",
    "prefix = 'deepar/bikerental'\n",
    "\n",
    "# This structure allows multiple training and test files for model development and testing\n",
    "\n",
    "if with_categories:\n",
    "    s3_data_path = \"{}/{}/data_with_categories\".format(bucket, prefix)\n",
    "else:\n",
    "    s3_data_path = \"{}/{}/data\".format(bucket,prefix)\n",
    "    \n",
    "s3_output_path = \"{}/{}/output\".format(bucket,prefix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6915813",
   "metadata": {},
   "outputs": [],
   "source": [
    "#s3_data_path, s3_output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d9b2f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that uploads files to s3 bucket\n",
    "\n",
    "def write_to_s3(filename, bucket, key):\n",
    "    with open(filename, 'rb') as f:\n",
    "        return boto3.Session().resource('s3').Bucket(bucket).Object(key).upload_fileobj(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5bfe036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload one or more training files and test files to s3\n",
    "\n",
    "if with_categories:\n",
    "    write_to_s3('train_with_categories.json', bucket, 'deepar/bikerental/data_with_categories/train/train_with_categories.json')\n",
    "    write_to_s3('test_with_categories.json', bucket, 'deepar/bikerental/data_with_categories/test/test_with_categories.json')\n",
    "else:\n",
    "    write_to_s3('train.json',bucket, 'deepar/bikerental/data/train/train.json')\n",
    "    write_to_s3('test.json', bucket, 'deepar/bikerental/data/test/test.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d108115f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use spont instance \n",
    "\n",
    "use_spot_instances = True\n",
    "max_run = 3600\n",
    "max_wait = 3600 if use_spot_instances else None \n",
    "\n",
    "job_name = base_job_name\n",
    "\n",
    "checkpoint_s3_uri = None\n",
    "\n",
    "if use_spot_instances:\n",
    "    checkpoint_s3_uri = f's3://{bucket}/{prefix}/checkpoints/{job_name}'\n",
    "    \n",
    "#print(f'Checkpoint uri: {checkpoint_s3_uri}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c356d19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish a session with AWS\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "97a66610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This role contains the permissions needed to train, deploy models\n",
    "# Sagemaker serveis is trusted to assume this role\n",
    "\n",
    "#print(role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0520b418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using DeepAR container 522234722520.dkr.ecr.us-east-1.amazonaws.com/forecasting-deepar:1\n"
     ]
    }
   ],
   "source": [
    "# SDK 2 uses image_uris.retrie to get container image\n",
    "\n",
    "container = sagemaker.image_uris.retrieve(\"forecasting-deepar\",sess.boto_region_name)\n",
    "\n",
    "print(f'Using DeepAR container {container}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3f2b4f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = \"H\" # Data consists hourly data\n",
    "\n",
    "prediction_length = 288 # need to predict 12 days of data in hours\n",
    "\n",
    "context_length = 288 # AWS recommends that context lenght to be same as prediction length to look past the same length as pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "99d5fcaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure training job\n",
    "\n",
    "estimator = sagemaker.estimator.Estimator(\n",
    "    container,\n",
    "    role,\n",
    "    instance_count = 1,\n",
    "    instance_type = 'ml.m5.xlarge',\n",
    "    output_path = 's3://'+s3_output_path,\n",
    "    sagemaker_session=sess,\n",
    "    base_job_name= job_name,\n",
    "    use_spot_instances = use_spot_instances,\n",
    "    max_run= max_run,\n",
    "    max_wait = max_wait,\n",
    "    checkpoint_s3_uri = checkpoint_s3_uri\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7942f0f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('H', 288, 288)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq, context_length, prediction_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9f362c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deepar hyperparameters\n",
    "\n",
    "hyperparameters = {\n",
    "    \"time_freq\" : freq,\n",
    "    \"epochs\":\"400\",\n",
    "    \"early_stopping_patience\":\"10\",\n",
    "    \"mini_batch_size\": \"64\",\n",
    "    \"learning_rate\" : \"5E-4\",\n",
    "    \"context_length\" : str(context_length),\n",
    "    \"prediction_length\" : str(prediction_length),\n",
    "    \"cardinality\" : \"auto\" if with_categories else ''\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "764698ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'time_freq': 'H',\n",
       " 'epochs': '400',\n",
       " 'early_stopping_patience': '10',\n",
       " 'mini_batch_size': '64',\n",
       " 'learning_rate': '5E-4',\n",
       " 'context_length': '288',\n",
       " 'prediction_length': '288',\n",
       " 'cardinality': ''}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "099439ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.set_hyperparameters(**hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f7964318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating data channels of train and test files\n",
    "\n",
    "data_channels = {\n",
    "    \"train\" : \"s3://{}/train/\".format(s3_data_path),\n",
    "    \"test\" : \"s3://{}/test\".format(s3_data_path)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e2d01842",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "21270264",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: deepar-biketrain-no-categories-2024-06-06-13-20-47-915\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-06 13:20:48 Starting - Starting the training job...\n",
      "2024-06-06 13:21:03 Starting - Preparing the instances for training...\n",
      "2024-06-06 13:21:36 Downloading - Downloading input data...\n",
      "2024-06-06 13:21:56 Downloading - Downloading the training image.........\n",
      "2024-06-06 13:23:42 Training - Training image download completed. Training in progress...\u001b[34mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[34mRunning default environment configuration script\u001b[0m\n",
      "\u001b[34mRunning custom environment configuration script\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python3.8/site-packages/mxnet/model.py:97: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if num_device is 1 and 'dist' not in kvstore:\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:23:54 INFO 139771206125376] Reading default configuration from /opt/amazon/lib/python3.8/site-packages/algorithm/resources/default-input.json: {'_kvstore': 'auto', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_tuning_objective_metric': '', 'cardinality': 'auto', 'dropout_rate': '0.10', 'early_stopping_patience': '', 'embedding_dimension': '10', 'learning_rate': '0.001', 'likelihood': 'student-t', 'mini_batch_size': '128', 'num_cells': '40', 'num_dynamic_feat': 'auto', 'num_eval_samples': '100', 'num_layers': '2', 'test_quantiles': '[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]'}\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:23:54 INFO 139771206125376] Merging with provided configuration from /opt/ml/input/config/hyperparameters.json: {'context_length': '288', 'early_stopping_patience': '10', 'epochs': '400', 'learning_rate': '5E-4', 'mini_batch_size': '64', 'prediction_length': '288', 'time_freq': 'H'}\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:23:54 INFO 139771206125376] Final configuration: {'_kvstore': 'auto', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_tuning_objective_metric': '', 'cardinality': 'auto', 'dropout_rate': '0.10', 'early_stopping_patience': '10', 'embedding_dimension': '10', 'learning_rate': '5E-4', 'likelihood': 'student-t', 'mini_batch_size': '64', 'num_cells': '40', 'num_dynamic_feat': 'auto', 'num_eval_samples': '100', 'num_layers': '2', 'test_quantiles': '[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', 'context_length': '288', 'epochs': '400', 'prediction_length': '288', 'time_freq': 'H'}\u001b[0m\n",
      "\u001b[34mProcess 7 is a worker.\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:23:54 INFO 139771206125376] Detected entry point for worker worker\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:23:54 INFO 139771206125376] Using early stopping with patience 10\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:23:54 INFO 139771206125376] random_seed is None\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:23:54 INFO 139771206125376] [cardinality=auto] `cat` field was NOT found in the file `/opt/ml/input/data/train/train.json` and will NOT be used for training.\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:23:54 INFO 139771206125376] [num_dynamic_feat=auto] `dynamic_feat` field was NOT found in the file `/opt/ml/input/data/train/train.json` and will NOT be used for training.\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:23:54 INFO 139771206125376] Training set statistics:\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:23:54 INFO 139771206125376] Integer time series\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:23:54 INFO 139771206125376] number of time series: 3\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:23:54 INFO 139771206125376] number of observations: 50904\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:23:54 INFO 139771206125376] mean target length: 16968.0\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:23:54 INFO 139771206125376] min/mean/max target: 0.0/79.57296086751532/977.0\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:23:54 INFO 139771206125376] mean abs(target): 79.57296086751532\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:23:54 INFO 139771206125376] contains missing values: yes (37.5%)\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:23:54 INFO 139771206125376] Small number of time series. Doing 214 passes over dataset with prob 0.9968847352024922 per epoch.\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:23:54 INFO 139771206125376] Test set statistics:\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:23:54 INFO 139771206125376] Integer time series\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:23:54 INFO 139771206125376] number of time series: 3\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:23:54 INFO 139771206125376] number of observations: 51768\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:23:54 INFO 139771206125376] mean target length: 17256.0\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:23:54 INFO 139771206125376] min/mean/max target: 0.0/80.57008190387884/977.0\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:23:54 INFO 139771206125376] mean abs(target): 80.57008190387884\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:23:54 INFO 139771206125376] contains missing values: yes (36.9%)\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python3.8/site-packages/algorithm/core/date_feature_set.py:44: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)\n",
      "  return index.weekofyear / 51.0 - 0.5\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:23:54 INFO 139771206125376] #memory_usage::<batchbuffer> = 203.9306640625 mb\u001b[0m\n",
      "\u001b[34m/opt/amazon/python3.8/lib/python3.8/subprocess.py:848: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used\n",
      "  self.stdout = io.open(c2pread, 'rb', bufsize)\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:23:55 INFO 139771206125376] nvidia-smi: took 0.031 seconds to run.\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:23:55 INFO 139771206125376] nvidia-smi identified 0 GPUs.\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:23:55 INFO 139771206125376] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:23:55 INFO 139771206125376] Create Store: local\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1717680235.0160944, \"EndTime\": 1717680249.5812674, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"get_graph.time\": {\"sum\": 14562.581062316895, \"count\": 1, \"min\": 14562.581062316895, \"max\": 14562.581062316895}}}\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:24:09 INFO 139771206125376] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:24:12 INFO 139771206125376] #memory_usage::<model> = 270 mb\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1717680249.5813391, \"EndTime\": 1717680252.2944522, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"initialize.time\": {\"sum\": 17278.229475021362, \"count\": 1, \"min\": 17278.229475021362, \"max\": 17278.229475021362}}}\u001b[0m\n",
      "\u001b[34m[13:24:15] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.3.x_Cuda_11.1.x.404.0/AL2_x86_64/generic-flavor/src/src/operator/nn/mkldnn/mkldnn_base.cc:74: Allocate 10240 bytes with malloc directly\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:24:16 INFO 139771206125376] Epoch[0] Batch[0] avg_epoch_loss=3.673511\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:24:16 INFO 139771206125376] #quality_metric: host=algo-1, epoch=0, batch=0 train loss <loss>=3.6735105514526367\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:24:25 INFO 139771206125376] Epoch[0] Batch[5] avg_epoch_loss=3.626525\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:24:25 INFO 139771206125376] #quality_metric: host=algo-1, epoch=0, batch=5 train loss <loss>=3.626524885495504\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:24:25 INFO 139771206125376] Epoch[0] Batch [5]#011Speed: 35.97 samples/sec#011loss=3.626525\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:24:33 INFO 139771206125376] Epoch[0] Batch[10] avg_epoch_loss=3.595369\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:24:33 INFO 139771206125376] #quality_metric: host=algo-1, epoch=0, batch=10 train loss <loss>=3.557982921600342\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:24:33 INFO 139771206125376] Epoch[0] Batch [10]#011Speed: 36.98 samples/sec#011loss=3.557983\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:24:33 INFO 139771206125376] processed a total of 656 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1717680252.2945216, \"EndTime\": 1717680273.986278, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"epochs\": {\"sum\": 400.0, \"count\": 1, \"min\": 400, \"max\": 400}, \"update.time\": {\"sum\": 21691.681146621704, \"count\": 1, \"min\": 21691.681146621704, \"max\": 21691.681146621704}}}\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:24:33 INFO 139771206125376] #throughput_metric: host=algo-1, train throughput=30.241868634896438 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:24:33 INFO 139771206125376] #progress_metric: host=algo-1, completed 0.25 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:24:33 INFO 139771206125376] #quality_metric: host=algo-1, epoch=0, train loss <loss>=3.5953694473613393\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:24:33 INFO 139771206125376] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:24:34 INFO 139771206125376] Saved checkpoint to \"/opt/ml/model/state_e6cebba4-5be7-4cdf-a119-fb050cf3d4e6-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1717680273.9863467, \"EndTime\": 1717680274.1668324, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 180.04250526428223, \"count\": 1, \"min\": 180.04250526428223, \"max\": 180.04250526428223}}}\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:24:37 INFO 139771206125376] Epoch[1] Batch[0] avg_epoch_loss=3.525924\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:24:37 INFO 139771206125376] #quality_metric: host=algo-1, epoch=1, batch=0 train loss <loss>=3.5259244441986084\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[06/06/2024 13:24:46 INFO 139771206125376] Epoch[1] Batch[5] avg_epoch_loss=3.497752\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:24:46 INFO 139771206125376] #quality_metric: host=algo-1, epoch=1, batch=5 train loss <loss>=3.4977524280548096\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:24:46 INFO 139771206125376] Epoch[1] Batch [5]#011Speed: 37.83 samples/sec#011loss=3.497752\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:24:52 INFO 139771206125376] processed a total of 617 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1717680274.1669016, \"EndTime\": 1717680292.9312084, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 18764.24527168274, \"count\": 1, \"min\": 18764.24527168274, \"max\": 18764.24527168274}}}\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:24:52 INFO 139771206125376] #throughput_metric: host=algo-1, train throughput=32.88120734561044 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:24:52 INFO 139771206125376] #progress_metric: host=algo-1, completed 0.5 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:24:52 INFO 139771206125376] #quality_metric: host=algo-1, epoch=1, train loss <loss>=3.4709096908569337\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:24:52 INFO 139771206125376] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:24:53 INFO 139771206125376] Saved checkpoint to \"/opt/ml/model/state_7a5c35e8-c785-4786-b8d6-880084ca3bde-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1717680292.9314437, \"EndTime\": 1717680293.1025562, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 170.51243782043457, \"count\": 1, \"min\": 170.51243782043457, \"max\": 170.51243782043457}}}\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:24:56 INFO 139771206125376] Epoch[2] Batch[0] avg_epoch_loss=3.369600\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:24:56 INFO 139771206125376] #quality_metric: host=algo-1, epoch=2, batch=0 train loss <loss>=3.3696000576019287\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:25:05 INFO 139771206125376] Epoch[2] Batch[5] avg_epoch_loss=3.293641\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:25:05 INFO 139771206125376] #quality_metric: host=algo-1, epoch=2, batch=5 train loss <loss>=3.293641448020935\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:25:05 INFO 139771206125376] Epoch[2] Batch [5]#011Speed: 34.86 samples/sec#011loss=3.293641\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:25:12 INFO 139771206125376] processed a total of 591 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1717680293.1026201, \"EndTime\": 1717680312.6252215, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 19522.542715072632, \"count\": 1, \"min\": 19522.542715072632, \"max\": 19522.542715072632}}}\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:25:12 INFO 139771206125376] #throughput_metric: host=algo-1, train throughput=30.272532182813617 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:25:12 INFO 139771206125376] #progress_metric: host=algo-1, completed 0.75 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:25:12 INFO 139771206125376] #quality_metric: host=algo-1, epoch=2, train loss <loss>=3.2828233003616334\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:25:12 INFO 139771206125376] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:25:12 INFO 139771206125376] Saved checkpoint to \"/opt/ml/model/state_a43da339-aa71-400c-8e9d-7f42e082b2d5-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1717680312.6252935, \"EndTime\": 1717680312.8007834, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 174.8664379119873, \"count\": 1, \"min\": 174.8664379119873, \"max\": 174.8664379119873}}}\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:25:16 INFO 139771206125376] Epoch[3] Batch[0] avg_epoch_loss=3.331583\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:25:16 INFO 139771206125376] #quality_metric: host=algo-1, epoch=3, batch=0 train loss <loss>=3.3315834999084473\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:25:24 INFO 139771206125376] Epoch[3] Batch[5] avg_epoch_loss=3.233616\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:25:24 INFO 139771206125376] #quality_metric: host=algo-1, epoch=3, batch=5 train loss <loss>=3.2336158752441406\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:25:24 INFO 139771206125376] Epoch[3] Batch [5]#011Speed: 37.85 samples/sec#011loss=3.233616\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:25:31 INFO 139771206125376] processed a total of 616 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1717680312.800846, \"EndTime\": 1717680331.51552, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 18714.622735977173, \"count\": 1, \"min\": 18714.622735977173, \"max\": 18714.622735977173}}}\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:25:31 INFO 139771206125376] #throughput_metric: host=algo-1, train throughput=32.91527003486085 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:25:31 INFO 139771206125376] #progress_metric: host=algo-1, completed 1.0 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:25:31 INFO 139771206125376] #quality_metric: host=algo-1, epoch=3, train loss <loss>=3.197541379928589\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:25:31 INFO 139771206125376] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:25:31 INFO 139771206125376] Saved checkpoint to \"/opt/ml/model/state_6bbf92fe-9af1-486e-ac30-4055caeadce5-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1717680331.515588, \"EndTime\": 1717680331.6841643, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 167.91653633117676, \"count\": 1, \"min\": 167.91653633117676, \"max\": 167.91653633117676}}}\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:25:35 INFO 139771206125376] Epoch[4] Batch[0] avg_epoch_loss=3.161047\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:25:35 INFO 139771206125376] #quality_metric: host=algo-1, epoch=4, batch=0 train loss <loss>=3.1610469818115234\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:25:43 INFO 139771206125376] Epoch[4] Batch[5] avg_epoch_loss=3.161087\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:25:43 INFO 139771206125376] #quality_metric: host=algo-1, epoch=4, batch=5 train loss <loss>=3.161086916923523\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:25:43 INFO 139771206125376] Epoch[4] Batch [5]#011Speed: 37.79 samples/sec#011loss=3.161087\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:25:52 INFO 139771206125376] Epoch[4] Batch[10] avg_epoch_loss=3.129530\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:25:52 INFO 139771206125376] #quality_metric: host=algo-1, epoch=4, batch=10 train loss <loss>=3.0916606903076174\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:25:52 INFO 139771206125376] Epoch[4] Batch [10]#011Speed: 37.29 samples/sec#011loss=3.091661\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:25:52 INFO 139771206125376] processed a total of 672 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1717680331.6842322, \"EndTime\": 1717680352.2927067, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 20608.417987823486, \"count\": 1, \"min\": 20608.417987823486, \"max\": 20608.417987823486}}}\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:25:52 INFO 139771206125376] #throughput_metric: host=algo-1, train throughput=32.607834333080746 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:25:52 INFO 139771206125376] #progress_metric: host=algo-1, completed 1.25 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:25:52 INFO 139771206125376] #quality_metric: host=algo-1, epoch=4, train loss <loss>=3.1295295411890205\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:25:52 INFO 139771206125376] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:25:52 INFO 139771206125376] Saved checkpoint to \"/opt/ml/model/state_4acac66d-cb9e-4d3a-acc8-13bd8bf0999f-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1717680352.2928035, \"EndTime\": 1717680352.464988, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 171.71669006347656, \"count\": 1, \"min\": 171.71669006347656, \"max\": 171.71669006347656}}}\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:25:55 INFO 139771206125376] Epoch[5] Batch[0] avg_epoch_loss=3.159604\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:25:55 INFO 139771206125376] #quality_metric: host=algo-1, epoch=5, batch=0 train loss <loss>=3.1596038341522217\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:26:04 INFO 139771206125376] Epoch[5] Batch[5] avg_epoch_loss=3.074891\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:26:04 INFO 139771206125376] #quality_metric: host=algo-1, epoch=5, batch=5 train loss <loss>=3.074891130129496\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:26:04 INFO 139771206125376] Epoch[5] Batch [5]#011Speed: 35.44 samples/sec#011loss=3.074891\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:26:13 INFO 139771206125376] Epoch[5] Batch[10] avg_epoch_loss=3.025030\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:26:13 INFO 139771206125376] #quality_metric: host=algo-1, epoch=5, batch=10 train loss <loss>=2.965195560455322\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:26:13 INFO 139771206125376] Epoch[5] Batch [10]#011Speed: 37.06 samples/sec#011loss=2.965196\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:26:13 INFO 139771206125376] processed a total of 643 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1717680352.4650505, \"EndTime\": 1717680373.568942, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 21103.835105895996, \"count\": 1, \"min\": 21103.835105895996, \"max\": 21103.835105895996}}}\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:26:13 INFO 139771206125376] #throughput_metric: host=algo-1, train throughput=30.468253597503548 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:26:13 INFO 139771206125376] #progress_metric: host=algo-1, completed 1.5 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:26:13 INFO 139771206125376] #quality_metric: host=algo-1, epoch=5, train loss <loss>=3.025029507550326\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:26:13 INFO 139771206125376] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:26:13 INFO 139771206125376] Saved checkpoint to \"/opt/ml/model/state_13fe6716-fbeb-450f-b945-22ddd0e2b456-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1717680373.5690086, \"EndTime\": 1717680373.743018, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 173.50387573242188, \"count\": 1, \"min\": 173.50387573242188, \"max\": 173.50387573242188}}}\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:26:17 INFO 139771206125376] Epoch[6] Batch[0] avg_epoch_loss=2.902656\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:26:17 INFO 139771206125376] #quality_metric: host=algo-1, epoch=6, batch=0 train loss <loss>=2.902656316757202\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[06/06/2024 13:26:25 INFO 139771206125376] Epoch[6] Batch[5] avg_epoch_loss=3.043598\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:26:25 INFO 139771206125376] #quality_metric: host=algo-1, epoch=6, batch=5 train loss <loss>=3.043597936630249\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:26:25 INFO 139771206125376] Epoch[6] Batch [5]#011Speed: 37.70 samples/sec#011loss=3.043598\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:26:34 INFO 139771206125376] Epoch[6] Batch[10] avg_epoch_loss=2.923261\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:26:34 INFO 139771206125376] #quality_metric: host=algo-1, epoch=6, batch=10 train loss <loss>=2.7788577556610106\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:26:34 INFO 139771206125376] Epoch[6] Batch [10]#011Speed: 37.31 samples/sec#011loss=2.778858\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:26:34 INFO 139771206125376] processed a total of 650 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1717680373.7430818, \"EndTime\": 1717680394.2405653, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 20497.43127822876, \"count\": 1, \"min\": 20497.43127822876, \"max\": 20497.43127822876}}}\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:26:34 INFO 139771206125376] #throughput_metric: host=algo-1, train throughput=31.71114564992151 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:26:34 INFO 139771206125376] #progress_metric: host=algo-1, completed 1.75 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:26:34 INFO 139771206125376] #quality_metric: host=algo-1, epoch=6, train loss <loss>=2.9232614907351406\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:26:34 INFO 139771206125376] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:26:34 INFO 139771206125376] Saved checkpoint to \"/opt/ml/model/state_e2d3d2ce-1658-44ed-9cd1-a22f481c6c02-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1717680394.2406304, \"EndTime\": 1717680394.4147975, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 173.74539375305176, \"count\": 1, \"min\": 173.74539375305176, \"max\": 173.74539375305176}}}\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:26:37 INFO 139771206125376] Epoch[7] Batch[0] avg_epoch_loss=2.895579\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:26:37 INFO 139771206125376] #quality_metric: host=algo-1, epoch=7, batch=0 train loss <loss>=2.8955788612365723\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:26:46 INFO 139771206125376] Epoch[7] Batch[5] avg_epoch_loss=2.933687\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:26:46 INFO 139771206125376] #quality_metric: host=algo-1, epoch=7, batch=5 train loss <loss>=2.9336872498194375\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:26:46 INFO 139771206125376] Epoch[7] Batch [5]#011Speed: 37.85 samples/sec#011loss=2.933687\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:26:53 INFO 139771206125376] processed a total of 618 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1717680394.4148605, \"EndTime\": 1717680413.1590228, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 18744.096755981445, \"count\": 1, \"min\": 18744.096755981445, \"max\": 18744.096755981445}}}\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:26:53 INFO 139771206125376] #throughput_metric: host=algo-1, train throughput=32.970195022615556 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:26:53 INFO 139771206125376] #progress_metric: host=algo-1, completed 2.0 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:26:53 INFO 139771206125376] #quality_metric: host=algo-1, epoch=7, train loss <loss>=2.8954166412353515\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:26:53 INFO 139771206125376] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:26:53 INFO 139771206125376] Saved checkpoint to \"/opt/ml/model/state_976ffe0e-fcbf-4529-a982-b055e3ba037b-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1717680413.1590912, \"EndTime\": 1717680413.3304873, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 170.7139015197754, \"count\": 1, \"min\": 170.7139015197754, \"max\": 170.7139015197754}}}\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:26:56 INFO 139771206125376] Epoch[8] Batch[0] avg_epoch_loss=2.906078\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:26:56 INFO 139771206125376] #quality_metric: host=algo-1, epoch=8, batch=0 train loss <loss>=2.9060781002044678\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:27:05 INFO 139771206125376] Epoch[8] Batch[5] avg_epoch_loss=2.872053\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:27:05 INFO 139771206125376] #quality_metric: host=algo-1, epoch=8, batch=5 train loss <loss>=2.872052788734436\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:27:05 INFO 139771206125376] Epoch[8] Batch [5]#011Speed: 35.33 samples/sec#011loss=2.872053\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:27:12 INFO 139771206125376] processed a total of 609 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1717680413.3305564, \"EndTime\": 1717680432.6733787, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 19342.76294708252, \"count\": 1, \"min\": 19342.76294708252, \"max\": 19342.76294708252}}}\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:27:12 INFO 139771206125376] #throughput_metric: host=algo-1, train throughput=31.484488879780304 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:27:12 INFO 139771206125376] #progress_metric: host=algo-1, completed 2.25 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:27:12 INFO 139771206125376] #quality_metric: host=algo-1, epoch=8, train loss <loss>=2.817157745361328\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:27:12 INFO 139771206125376] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:27:12 INFO 139771206125376] Saved checkpoint to \"/opt/ml/model/state_1084378f-0f3b-490e-9b20-a441a5b5e524-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1717680432.6734436, \"EndTime\": 1717680432.8437085, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 169.8596477508545, \"count\": 1, \"min\": 169.8596477508545, \"max\": 169.8596477508545}}}\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:27:16 INFO 139771206125376] Epoch[9] Batch[0] avg_epoch_loss=2.950123\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:27:16 INFO 139771206125376] #quality_metric: host=algo-1, epoch=9, batch=0 train loss <loss>=2.950122594833374\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:27:24 INFO 139771206125376] Epoch[9] Batch[5] avg_epoch_loss=2.917852\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:27:24 INFO 139771206125376] #quality_metric: host=algo-1, epoch=9, batch=5 train loss <loss>=2.9178520043691\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:27:24 INFO 139771206125376] Epoch[9] Batch [5]#011Speed: 37.80 samples/sec#011loss=2.917852\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:27:33 INFO 139771206125376] Epoch[9] Batch[10] avg_epoch_loss=2.911372\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:27:33 INFO 139771206125376] #quality_metric: host=algo-1, epoch=9, batch=10 train loss <loss>=2.903595781326294\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:27:33 INFO 139771206125376] Epoch[9] Batch [10]#011Speed: 36.94 samples/sec#011loss=2.903596\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:27:33 INFO 139771206125376] processed a total of 670 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1717680432.8437724, \"EndTime\": 1717680453.3780382, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 20534.212827682495, \"count\": 1, \"min\": 20534.212827682495, \"max\": 20534.212827682495}}}\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:27:33 INFO 139771206125376] #throughput_metric: host=algo-1, train throughput=32.628333535631256 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:27:33 INFO 139771206125376] #progress_metric: host=algo-1, completed 2.5 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:27:33 INFO 139771206125376] #quality_metric: host=algo-1, epoch=9, train loss <loss>=2.9113719029860063\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:27:33 INFO 139771206125376] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:27:36 INFO 139771206125376] Epoch[10] Batch[0] avg_epoch_loss=2.707431\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:27:36 INFO 139771206125376] #quality_metric: host=algo-1, epoch=10, batch=0 train loss <loss>=2.707430839538574\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:27:45 INFO 139771206125376] Epoch[10] Batch[5] avg_epoch_loss=2.787993\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:27:45 INFO 139771206125376] #quality_metric: host=algo-1, epoch=10, batch=5 train loss <loss>=2.787992835044861\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:27:45 INFO 139771206125376] Epoch[10] Batch [5]#011Speed: 37.85 samples/sec#011loss=2.787993\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:27:53 INFO 139771206125376] Epoch[10] Batch[10] avg_epoch_loss=2.800372\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:27:53 INFO 139771206125376] #quality_metric: host=algo-1, epoch=10, batch=10 train loss <loss>=2.8152260303497316\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:27:53 INFO 139771206125376] Epoch[10] Batch [10]#011Speed: 37.20 samples/sec#011loss=2.815226\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:27:53 INFO 139771206125376] processed a total of 661 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1717680453.3780968, \"EndTime\": 1717680473.8251739, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 20446.50411605835, \"count\": 1, \"min\": 20446.50411605835, \"max\": 20446.50411605835}}}\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:27:53 INFO 139771206125376] #throughput_metric: host=algo-1, train throughput=32.32812045657543 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:27:53 INFO 139771206125376] #progress_metric: host=algo-1, completed 2.75 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:27:53 INFO 139771206125376] #quality_metric: host=algo-1, epoch=10, train loss <loss>=2.8003715601834385\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:27:53 INFO 139771206125376] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:27:53 INFO 139771206125376] Saved checkpoint to \"/opt/ml/model/state_51986e60-e31b-4db6-b029-16604b706d68-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1717680473.8252363, \"EndTime\": 1717680473.996262, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 170.48954963684082, \"count\": 1, \"min\": 170.48954963684082, \"max\": 170.48954963684082}}}\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:27:57 INFO 139771206125376] Epoch[11] Batch[0] avg_epoch_loss=2.859401\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:27:57 INFO 139771206125376] #quality_metric: host=algo-1, epoch=11, batch=0 train loss <loss>=2.859401226043701\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[06/06/2024 13:28:06 INFO 139771206125376] Epoch[11] Batch[5] avg_epoch_loss=2.812002\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:28:06 INFO 139771206125376] #quality_metric: host=algo-1, epoch=11, batch=5 train loss <loss>=2.812001665433248\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:28:06 INFO 139771206125376] Epoch[11] Batch [5]#011Speed: 34.96 samples/sec#011loss=2.812002\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:28:15 INFO 139771206125376] Epoch[11] Batch[10] avg_epoch_loss=2.873223\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:28:15 INFO 139771206125376] #quality_metric: host=algo-1, epoch=11, batch=10 train loss <loss>=2.94668869972229\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:28:15 INFO 139771206125376] Epoch[11] Batch [10]#011Speed: 37.13 samples/sec#011loss=2.946689\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:28:15 INFO 139771206125376] processed a total of 644 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1717680473.9963236, \"EndTime\": 1717680495.1907887, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 21194.412231445312, \"count\": 1, \"min\": 21194.412231445312, \"max\": 21194.412231445312}}}\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:28:15 INFO 139771206125376] #throughput_metric: host=algo-1, train throughput=30.385229192892943 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:28:15 INFO 139771206125376] #progress_metric: host=algo-1, completed 3.0 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:28:15 INFO 139771206125376] #quality_metric: host=algo-1, epoch=11, train loss <loss>=2.8732230446555396\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:28:15 INFO 139771206125376] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:28:18 INFO 139771206125376] Epoch[12] Batch[0] avg_epoch_loss=2.795502\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:28:18 INFO 139771206125376] #quality_metric: host=algo-1, epoch=12, batch=0 train loss <loss>=2.7955024242401123\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:28:27 INFO 139771206125376] Epoch[12] Batch[5] avg_epoch_loss=2.848192\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:28:27 INFO 139771206125376] #quality_metric: host=algo-1, epoch=12, batch=5 train loss <loss>=2.848191976547241\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:28:27 INFO 139771206125376] Epoch[12] Batch [5]#011Speed: 37.79 samples/sec#011loss=2.848192\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:28:35 INFO 139771206125376] Epoch[12] Batch[10] avg_epoch_loss=2.820516\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:28:35 INFO 139771206125376] #quality_metric: host=algo-1, epoch=12, batch=10 train loss <loss>=2.7873050212860107\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:28:35 INFO 139771206125376] Epoch[12] Batch [10]#011Speed: 36.99 samples/sec#011loss=2.787305\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:28:35 INFO 139771206125376] processed a total of 675 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1717680495.190856, \"EndTime\": 1717680515.6892407, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 20497.987270355225, \"count\": 1, \"min\": 20497.987270355225, \"max\": 20497.987270355225}}}\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:28:35 INFO 139771206125376] #throughput_metric: host=algo-1, train throughput=32.9298693645642 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:28:35 INFO 139771206125376] #progress_metric: host=algo-1, completed 3.25 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:28:35 INFO 139771206125376] #quality_metric: host=algo-1, epoch=12, train loss <loss>=2.8205160877921363\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:28:35 INFO 139771206125376] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:28:39 INFO 139771206125376] Epoch[13] Batch[0] avg_epoch_loss=2.757625\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:28:39 INFO 139771206125376] #quality_metric: host=algo-1, epoch=13, batch=0 train loss <loss>=2.757624626159668\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:28:47 INFO 139771206125376] Epoch[13] Batch[5] avg_epoch_loss=2.820131\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:28:47 INFO 139771206125376] #quality_metric: host=algo-1, epoch=13, batch=5 train loss <loss>=2.8201311031977334\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:28:47 INFO 139771206125376] Epoch[13] Batch [5]#011Speed: 37.75 samples/sec#011loss=2.820131\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:28:56 INFO 139771206125376] Epoch[13] Batch[10] avg_epoch_loss=2.779178\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:28:56 INFO 139771206125376] #quality_metric: host=algo-1, epoch=13, batch=10 train loss <loss>=2.730034351348877\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:28:56 INFO 139771206125376] Epoch[13] Batch [10]#011Speed: 37.38 samples/sec#011loss=2.730034\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:28:56 INFO 139771206125376] processed a total of 653 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1717680515.6893313, \"EndTime\": 1717680536.1661556, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 20476.40299797058, \"count\": 1, \"min\": 20476.40299797058, \"max\": 20476.40299797058}}}\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:28:56 INFO 139771206125376] #throughput_metric: host=algo-1, train throughput=31.890212217721032 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:28:56 INFO 139771206125376] #progress_metric: host=algo-1, completed 3.5 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:28:56 INFO 139771206125376] #quality_metric: host=algo-1, epoch=13, train loss <loss>=2.779178034175526\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:28:56 INFO 139771206125376] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:28:56 INFO 139771206125376] Saved checkpoint to \"/opt/ml/model/state_1052a889-dc30-40cd-b383-13da9af2bcb1-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1717680536.1662223, \"EndTime\": 1717680536.3386946, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 171.90146446228027, \"count\": 1, \"min\": 171.90146446228027, \"max\": 171.90146446228027}}}\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:28:59 INFO 139771206125376] Epoch[14] Batch[0] avg_epoch_loss=2.909298\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:28:59 INFO 139771206125376] #quality_metric: host=algo-1, epoch=14, batch=0 train loss <loss>=2.9092977046966553\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:29:08 INFO 139771206125376] Epoch[14] Batch[5] avg_epoch_loss=2.847463\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:29:08 INFO 139771206125376] #quality_metric: host=algo-1, epoch=14, batch=5 train loss <loss>=2.847462773323059\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:29:08 INFO 139771206125376] Epoch[14] Batch [5]#011Speed: 35.82 samples/sec#011loss=2.847463\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:29:15 INFO 139771206125376] processed a total of 598 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1717680536.33876, \"EndTime\": 1717680555.5704627, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 19231.640815734863, \"count\": 1, \"min\": 19231.640815734863, \"max\": 19231.640815734863}}}\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:29:15 INFO 139771206125376] #throughput_metric: host=algo-1, train throughput=31.09437985854762 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:29:15 INFO 139771206125376] #progress_metric: host=algo-1, completed 3.75 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:29:15 INFO 139771206125376] #quality_metric: host=algo-1, epoch=14, train loss <loss>=2.9309115648269652\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:29:15 INFO 139771206125376] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:29:19 INFO 139771206125376] Epoch[15] Batch[0] avg_epoch_loss=2.559165\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:29:19 INFO 139771206125376] #quality_metric: host=algo-1, epoch=15, batch=0 train loss <loss>=2.5591652393341064\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:29:27 INFO 139771206125376] Epoch[15] Batch[5] avg_epoch_loss=2.714922\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:29:27 INFO 139771206125376] #quality_metric: host=algo-1, epoch=15, batch=5 train loss <loss>=2.714921514193217\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:29:27 INFO 139771206125376] Epoch[15] Batch [5]#011Speed: 37.75 samples/sec#011loss=2.714922\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:29:34 INFO 139771206125376] processed a total of 614 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1717680555.5705576, \"EndTime\": 1717680574.3250134, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 18753.99947166443, \"count\": 1, \"min\": 18753.99947166443, \"max\": 18753.99947166443}}}\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:29:34 INFO 139771206125376] #throughput_metric: host=algo-1, train throughput=32.73937470897163 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:29:34 INFO 139771206125376] #progress_metric: host=algo-1, completed 4.0 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:29:34 INFO 139771206125376] #quality_metric: host=algo-1, epoch=15, train loss <loss>=2.7317256927490234\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:29:34 INFO 139771206125376] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:29:34 INFO 139771206125376] Saved checkpoint to \"/opt/ml/model/state_b5ec599a-1615-45b4-8de1-2ff26d1f3b39-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1717680574.3251464, \"EndTime\": 1717680574.4997222, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 174.0875244140625, \"count\": 1, \"min\": 174.0875244140625, \"max\": 174.0875244140625}}}\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:29:37 INFO 139771206125376] Epoch[16] Batch[0] avg_epoch_loss=2.843157\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:29:37 INFO 139771206125376] #quality_metric: host=algo-1, epoch=16, batch=0 train loss <loss>=2.843156576156616\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[06/06/2024 13:29:46 INFO 139771206125376] Epoch[16] Batch[5] avg_epoch_loss=2.781484\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:29:46 INFO 139771206125376] #quality_metric: host=algo-1, epoch=16, batch=5 train loss <loss>=2.7814835707346597\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:29:46 INFO 139771206125376] Epoch[16] Batch [5]#011Speed: 37.85 samples/sec#011loss=2.781484\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:29:53 INFO 139771206125376] processed a total of 617 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1717680574.4997854, \"EndTime\": 1717680593.227391, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 18727.54955291748, \"count\": 1, \"min\": 18727.54955291748, \"max\": 18727.54955291748}}}\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:29:53 INFO 139771206125376] #throughput_metric: host=algo-1, train throughput=32.945942966930666 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:29:53 INFO 139771206125376] #progress_metric: host=algo-1, completed 4.25 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:29:53 INFO 139771206125376] #quality_metric: host=algo-1, epoch=16, train loss <loss>=2.7396628141403196\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:29:53 INFO 139771206125376] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:29:56 INFO 139771206125376] Epoch[17] Batch[0] avg_epoch_loss=2.798771\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:29:56 INFO 139771206125376] #quality_metric: host=algo-1, epoch=17, batch=0 train loss <loss>=2.798771381378174\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:30:05 INFO 139771206125376] Epoch[17] Batch[5] avg_epoch_loss=2.760136\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:30:05 INFO 139771206125376] #quality_metric: host=algo-1, epoch=17, batch=5 train loss <loss>=2.7601357301076255\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:30:05 INFO 139771206125376] Epoch[17] Batch [5]#011Speed: 34.89 samples/sec#011loss=2.760136\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:30:12 INFO 139771206125376] processed a total of 619 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1717680593.2274585, \"EndTime\": 1717680612.7036512, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 19475.68702697754, \"count\": 1, \"min\": 19475.68702697754, \"max\": 19475.68702697754}}}\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:30:12 INFO 139771206125376] #throughput_metric: host=algo-1, train throughput=31.78306086664209 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:30:12 INFO 139771206125376] #progress_metric: host=algo-1, completed 4.5 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:30:12 INFO 139771206125376] #quality_metric: host=algo-1, epoch=17, train loss <loss>=2.7318480968475343\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:30:12 INFO 139771206125376] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:30:16 INFO 139771206125376] Epoch[18] Batch[0] avg_epoch_loss=2.753491\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:30:16 INFO 139771206125376] #quality_metric: host=algo-1, epoch=18, batch=0 train loss <loss>=2.7534914016723633\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:30:24 INFO 139771206125376] Epoch[18] Batch[5] avg_epoch_loss=2.706190\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:30:24 INFO 139771206125376] #quality_metric: host=algo-1, epoch=18, batch=5 train loss <loss>=2.706189513206482\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:30:24 INFO 139771206125376] Epoch[18] Batch [5]#011Speed: 37.76 samples/sec#011loss=2.706190\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:30:33 INFO 139771206125376] Epoch[18] Batch[10] avg_epoch_loss=2.802821\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:30:33 INFO 139771206125376] #quality_metric: host=algo-1, epoch=18, batch=10 train loss <loss>=2.918779420852661\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:30:33 INFO 139771206125376] Epoch[18] Batch [10]#011Speed: 37.46 samples/sec#011loss=2.918779\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:30:33 INFO 139771206125376] processed a total of 655 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1717680612.703718, \"EndTime\": 1717680633.146442, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 20442.262172698975, \"count\": 1, \"min\": 20442.262172698975, \"max\": 20442.262172698975}}}\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:30:33 INFO 139771206125376] #throughput_metric: host=algo-1, train throughput=32.04132499161753 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:30:33 INFO 139771206125376] #progress_metric: host=algo-1, completed 4.75 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:30:33 INFO 139771206125376] #quality_metric: host=algo-1, epoch=18, train loss <loss>=2.8028212894092905\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:30:33 INFO 139771206125376] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:30:36 INFO 139771206125376] Epoch[19] Batch[0] avg_epoch_loss=2.714265\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:30:36 INFO 139771206125376] #quality_metric: host=algo-1, epoch=19, batch=0 train loss <loss>=2.7142651081085205\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:30:45 INFO 139771206125376] Epoch[19] Batch[5] avg_epoch_loss=2.727899\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:30:45 INFO 139771206125376] #quality_metric: host=algo-1, epoch=19, batch=5 train loss <loss>=2.727898875872294\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:30:45 INFO 139771206125376] Epoch[19] Batch [5]#011Speed: 37.60 samples/sec#011loss=2.727899\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:30:51 INFO 139771206125376] processed a total of 626 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1717680633.1465018, \"EndTime\": 1717680651.9519362, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 18805.007696151733, \"count\": 1, \"min\": 18805.007696151733, \"max\": 18805.007696151733}}}\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:30:51 INFO 139771206125376] #throughput_metric: host=algo-1, train throughput=33.288831791027576 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:30:51 INFO 139771206125376] #progress_metric: host=algo-1, completed 5.0 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:30:51 INFO 139771206125376] #quality_metric: host=algo-1, epoch=19, train loss <loss>=2.7223172903060915\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:30:51 INFO 139771206125376] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:30:52 INFO 139771206125376] Saved checkpoint to \"/opt/ml/model/state_63d59fb2-206e-4003-ad82-3768f7f7ad3e-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1717680651.9520023, \"EndTime\": 1717680652.1235085, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 171.04458808898926, \"count\": 1, \"min\": 171.04458808898926, \"max\": 171.04458808898926}}}\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:30:55 INFO 139771206125376] Epoch[20] Batch[0] avg_epoch_loss=2.731635\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:30:55 INFO 139771206125376] #quality_metric: host=algo-1, epoch=20, batch=0 train loss <loss>=2.731635332107544\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:31:04 INFO 139771206125376] Epoch[20] Batch[5] avg_epoch_loss=2.706789\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:31:04 INFO 139771206125376] #quality_metric: host=algo-1, epoch=20, batch=5 train loss <loss>=2.706788976987203\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:31:04 INFO 139771206125376] Epoch[20] Batch [5]#011Speed: 35.40 samples/sec#011loss=2.706789\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:31:11 INFO 139771206125376] processed a total of 619 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1717680652.1235719, \"EndTime\": 1717680671.9052455, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 19781.618356704712, \"count\": 1, \"min\": 19781.618356704712, \"max\": 19781.618356704712}}}\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:31:11 INFO 139771206125376] #throughput_metric: host=algo-1, train throughput=31.291442180907577 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:31:11 INFO 139771206125376] #progress_metric: host=algo-1, completed 5.25 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:31:11 INFO 139771206125376] #quality_metric: host=algo-1, epoch=20, train loss <loss>=2.646182727813721\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:31:11 INFO 139771206125376] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:31:12 INFO 139771206125376] Saved checkpoint to \"/opt/ml/model/state_7a66db46-ae6b-4986-95df-09a976c481ed-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1717680671.9053636, \"EndTime\": 1717680672.0762835, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 170.45831680297852, \"count\": 1, \"min\": 170.45831680297852, \"max\": 170.45831680297852}}}\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:31:15 INFO 139771206125376] Epoch[21] Batch[0] avg_epoch_loss=2.714566\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:31:15 INFO 139771206125376] #quality_metric: host=algo-1, epoch=21, batch=0 train loss <loss>=2.714566469192505\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[06/06/2024 13:31:23 INFO 139771206125376] Epoch[21] Batch[5] avg_epoch_loss=2.703329\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:31:23 INFO 139771206125376] #quality_metric: host=algo-1, epoch=21, batch=5 train loss <loss>=2.70332940419515\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:31:23 INFO 139771206125376] Epoch[21] Batch [5]#011Speed: 37.70 samples/sec#011loss=2.703329\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:31:32 INFO 139771206125376] Epoch[21] Batch[10] avg_epoch_loss=2.776821\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:31:32 INFO 139771206125376] #quality_metric: host=algo-1, epoch=21, batch=10 train loss <loss>=2.8650105953216554\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:31:32 INFO 139771206125376] Epoch[21] Batch [10]#011Speed: 36.88 samples/sec#011loss=2.865011\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:31:32 INFO 139771206125376] processed a total of 669 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1717680672.0763466, \"EndTime\": 1717680692.6257646, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 20549.362182617188, \"count\": 1, \"min\": 20549.362182617188, \"max\": 20549.362182617188}}}\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:31:32 INFO 139771206125376] #throughput_metric: host=algo-1, train throughput=32.55559746134527 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:31:32 INFO 139771206125376] #progress_metric: host=algo-1, completed 5.5 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:31:32 INFO 139771206125376] #quality_metric: host=algo-1, epoch=21, train loss <loss>=2.7768208547071977\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:31:32 INFO 139771206125376] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:31:36 INFO 139771206125376] Epoch[22] Batch[0] avg_epoch_loss=2.695596\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:31:36 INFO 139771206125376] #quality_metric: host=algo-1, epoch=22, batch=0 train loss <loss>=2.6955959796905518\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:31:44 INFO 139771206125376] Epoch[22] Batch[5] avg_epoch_loss=2.637965\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:31:44 INFO 139771206125376] #quality_metric: host=algo-1, epoch=22, batch=5 train loss <loss>=2.637964685757955\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:31:44 INFO 139771206125376] Epoch[22] Batch [5]#011Speed: 37.73 samples/sec#011loss=2.637965\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:31:51 INFO 139771206125376] processed a total of 635 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1717680692.6258345, \"EndTime\": 1717680711.398707, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 18772.445917129517, \"count\": 1, \"min\": 18772.445917129517, \"max\": 18772.445917129517}}}\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:31:51 INFO 139771206125376] #throughput_metric: host=algo-1, train throughput=33.825963180876464 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:31:51 INFO 139771206125376] #progress_metric: host=algo-1, completed 5.75 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:31:51 INFO 139771206125376] #quality_metric: host=algo-1, epoch=22, train loss <loss>=2.6606157779693604\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:31:51 INFO 139771206125376] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:31:54 INFO 139771206125376] Epoch[23] Batch[0] avg_epoch_loss=2.671649\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:31:54 INFO 139771206125376] #quality_metric: host=algo-1, epoch=23, batch=0 train loss <loss>=2.6716489791870117\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:32:03 INFO 139771206125376] Epoch[23] Batch[5] avg_epoch_loss=2.664772\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:32:03 INFO 139771206125376] #quality_metric: host=algo-1, epoch=23, batch=5 train loss <loss>=2.6647719939549765\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:32:03 INFO 139771206125376] Epoch[23] Batch [5]#011Speed: 35.29 samples/sec#011loss=2.664772\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:32:12 INFO 139771206125376] Epoch[23] Batch[10] avg_epoch_loss=2.707661\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:32:12 INFO 139771206125376] #quality_metric: host=algo-1, epoch=23, batch=10 train loss <loss>=2.759128284454346\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:32:12 INFO 139771206125376] Epoch[23] Batch [10]#011Speed: 37.14 samples/sec#011loss=2.759128\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:32:12 INFO 139771206125376] processed a total of 677 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1717680711.3987782, \"EndTime\": 1717680732.4399595, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 21040.78507423401, \"count\": 1, \"min\": 21040.78507423401, \"max\": 21040.78507423401}}}\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:32:12 INFO 139771206125376] #throughput_metric: host=algo-1, train throughput=32.17546841665881 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:32:12 INFO 139771206125376] #progress_metric: host=algo-1, completed 6.0 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:32:12 INFO 139771206125376] #quality_metric: host=algo-1, epoch=23, train loss <loss>=2.7076612169092353\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:32:12 INFO 139771206125376] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:32:15 INFO 139771206125376] Epoch[24] Batch[0] avg_epoch_loss=2.623610\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:32:15 INFO 139771206125376] #quality_metric: host=algo-1, epoch=24, batch=0 train loss <loss>=2.623609781265259\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:32:24 INFO 139771206125376] Epoch[24] Batch[5] avg_epoch_loss=2.645645\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:32:24 INFO 139771206125376] #quality_metric: host=algo-1, epoch=24, batch=5 train loss <loss>=2.645644505818685\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:32:24 INFO 139771206125376] Epoch[24] Batch [5]#011Speed: 37.77 samples/sec#011loss=2.645645\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:32:31 INFO 139771206125376] processed a total of 598 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1717680732.4400208, \"EndTime\": 1717680751.167094, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 18726.71604156494, \"count\": 1, \"min\": 18726.71604156494, \"max\": 18726.71604156494}}}\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:32:31 INFO 139771206125376] #throughput_metric: host=algo-1, train throughput=31.9328075640379 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:32:31 INFO 139771206125376] #progress_metric: host=algo-1, completed 6.25 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:32:31 INFO 139771206125376] #quality_metric: host=algo-1, epoch=24, train loss <loss>=2.6394490003585815\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:32:31 INFO 139771206125376] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:32:31 INFO 139771206125376] Saved checkpoint to \"/opt/ml/model/state_1c2c5362-a4fa-41a0-89ab-a64cdcfe0fd3-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1717680751.167166, \"EndTime\": 1717680751.3466065, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 179.04996871948242, \"count\": 1, \"min\": 179.04996871948242, \"max\": 179.04996871948242}}}\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:32:34 INFO 139771206125376] Epoch[25] Batch[0] avg_epoch_loss=2.643024\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:32:34 INFO 139771206125376] #quality_metric: host=algo-1, epoch=25, batch=0 train loss <loss>=2.643023729324341\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:32:43 INFO 139771206125376] Epoch[25] Batch[5] avg_epoch_loss=2.635941\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:32:43 INFO 139771206125376] #quality_metric: host=algo-1, epoch=25, batch=5 train loss <loss>=2.6359408299128213\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:32:43 INFO 139771206125376] Epoch[25] Batch [5]#011Speed: 37.79 samples/sec#011loss=2.635941\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:32:51 INFO 139771206125376] Epoch[25] Batch[10] avg_epoch_loss=2.664580\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:32:51 INFO 139771206125376] #quality_metric: host=algo-1, epoch=25, batch=10 train loss <loss>=2.698947620391846\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:32:51 INFO 139771206125376] Epoch[25] Batch [10]#011Speed: 37.28 samples/sec#011loss=2.698948\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:32:51 INFO 139771206125376] processed a total of 675 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1717680751.3466678, \"EndTime\": 1717680771.795986, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 20449.264764785767, \"count\": 1, \"min\": 20449.264764785767, \"max\": 20449.264764785767}}}\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:32:51 INFO 139771206125376] #throughput_metric: host=algo-1, train throughput=33.008366620094506 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:32:51 INFO 139771206125376] #progress_metric: host=algo-1, completed 6.5 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:32:51 INFO 139771206125376] #quality_metric: host=algo-1, epoch=25, train loss <loss>=2.6645802801305596\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:32:51 INFO 139771206125376] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:32:55 INFO 139771206125376] Epoch[26] Batch[0] avg_epoch_loss=2.541227\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:32:55 INFO 139771206125376] #quality_metric: host=algo-1, epoch=26, batch=0 train loss <loss>=2.541227340698242\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[06/06/2024 13:33:04 INFO 139771206125376] Epoch[26] Batch[5] avg_epoch_loss=2.628748\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:33:04 INFO 139771206125376] #quality_metric: host=algo-1, epoch=26, batch=5 train loss <loss>=2.628748138745626\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:33:04 INFO 139771206125376] Epoch[26] Batch [5]#011Speed: 35.97 samples/sec#011loss=2.628748\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:33:12 INFO 139771206125376] Epoch[26] Batch[10] avg_epoch_loss=2.614921\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:33:12 INFO 139771206125376] #quality_metric: host=algo-1, epoch=26, batch=10 train loss <loss>=2.598328161239624\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:33:12 INFO 139771206125376] Epoch[26] Batch [10]#011Speed: 37.47 samples/sec#011loss=2.598328\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:33:12 INFO 139771206125376] processed a total of 651 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1717680771.7960544, \"EndTime\": 1717680792.6719594, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 20875.478982925415, \"count\": 1, \"min\": 20875.478982925415, \"max\": 20875.478982925415}}}\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:33:12 INFO 139771206125376] #throughput_metric: host=algo-1, train throughput=31.1847728682512 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:33:12 INFO 139771206125376] #progress_metric: host=algo-1, completed 6.75 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:33:12 INFO 139771206125376] #quality_metric: host=algo-1, epoch=26, train loss <loss>=2.614920876242898\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:33:12 INFO 139771206125376] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:33:12 INFO 139771206125376] Saved checkpoint to \"/opt/ml/model/state_3dcd551d-1aef-49a5-b44f-0bf86ad0f8e4-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1717680792.672023, \"EndTime\": 1717680792.8410459, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 168.72000694274902, \"count\": 1, \"min\": 168.72000694274902, \"max\": 168.72000694274902}}}\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:33:16 INFO 139771206125376] Epoch[27] Batch[0] avg_epoch_loss=2.621848\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:33:16 INFO 139771206125376] #quality_metric: host=algo-1, epoch=27, batch=0 train loss <loss>=2.6218483448028564\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:33:24 INFO 139771206125376] Epoch[27] Batch[5] avg_epoch_loss=2.620371\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:33:24 INFO 139771206125376] #quality_metric: host=algo-1, epoch=27, batch=5 train loss <loss>=2.6203712622324624\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:33:24 INFO 139771206125376] Epoch[27] Batch [5]#011Speed: 37.59 samples/sec#011loss=2.620371\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:33:31 INFO 139771206125376] processed a total of 636 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1717680792.8411095, \"EndTime\": 1717680811.6075745, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 18766.408920288086, \"count\": 1, \"min\": 18766.408920288086, \"max\": 18766.408920288086}}}\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:33:31 INFO 139771206125376] #throughput_metric: host=algo-1, train throughput=33.89013273468557 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:33:31 INFO 139771206125376] #progress_metric: host=algo-1, completed 7.0 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:33:31 INFO 139771206125376] #quality_metric: host=algo-1, epoch=27, train loss <loss>=2.630847954750061\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:33:31 INFO 139771206125376] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:33:34 INFO 139771206125376] Epoch[28] Batch[0] avg_epoch_loss=2.510900\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:33:34 INFO 139771206125376] #quality_metric: host=algo-1, epoch=28, batch=0 train loss <loss>=2.5109002590179443\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:33:43 INFO 139771206125376] Epoch[28] Batch[5] avg_epoch_loss=2.662642\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:33:43 INFO 139771206125376] #quality_metric: host=algo-1, epoch=28, batch=5 train loss <loss>=2.662641723950704\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:33:43 INFO 139771206125376] Epoch[28] Batch [5]#011Speed: 37.65 samples/sec#011loss=2.662642\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:33:52 INFO 139771206125376] Epoch[28] Batch[10] avg_epoch_loss=2.698299\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:33:52 INFO 139771206125376] #quality_metric: host=algo-1, epoch=28, batch=10 train loss <loss>=2.741087055206299\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:33:52 INFO 139771206125376] Epoch[28] Batch [10]#011Speed: 37.17 samples/sec#011loss=2.741087\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:33:52 INFO 139771206125376] processed a total of 668 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1717680811.6076438, \"EndTime\": 1717680832.0706935, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 20462.592601776123, \"count\": 1, \"min\": 20462.592601776123, \"max\": 20462.592601776123}}}\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:33:52 INFO 139771206125376] #throughput_metric: host=algo-1, train throughput=32.64476965974065 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:33:52 INFO 139771206125376] #progress_metric: host=algo-1, completed 7.25 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:33:52 INFO 139771206125376] #quality_metric: host=algo-1, epoch=28, train loss <loss>=2.698298692703247\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:33:52 INFO 139771206125376] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:33:55 INFO 139771206125376] Epoch[29] Batch[0] avg_epoch_loss=2.557144\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:33:55 INFO 139771206125376] #quality_metric: host=algo-1, epoch=29, batch=0 train loss <loss>=2.5571436882019043\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:34:04 INFO 139771206125376] Epoch[29] Batch[5] avg_epoch_loss=2.655187\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:34:04 INFO 139771206125376] #quality_metric: host=algo-1, epoch=29, batch=5 train loss <loss>=2.6551872889200845\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:34:04 INFO 139771206125376] Epoch[29] Batch [5]#011Speed: 36.05 samples/sec#011loss=2.655187\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:34:12 INFO 139771206125376] Epoch[29] Batch[10] avg_epoch_loss=2.596176\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:34:12 INFO 139771206125376] #quality_metric: host=algo-1, epoch=29, batch=10 train loss <loss>=2.5253620624542235\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:34:12 INFO 139771206125376] Epoch[29] Batch [10]#011Speed: 37.35 samples/sec#011loss=2.525362\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:34:12 INFO 139771206125376] processed a total of 641 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1717680832.0707664, \"EndTime\": 1717680852.9763138, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 20905.124187469482, \"count\": 1, \"min\": 20905.124187469482, \"max\": 20905.124187469482}}}\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:34:12 INFO 139771206125376] #throughput_metric: host=algo-1, train throughput=30.66220169561574 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:34:12 INFO 139771206125376] #progress_metric: host=algo-1, completed 7.5 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:34:12 INFO 139771206125376] #quality_metric: host=algo-1, epoch=29, train loss <loss>=2.5961758223446934\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:34:12 INFO 139771206125376] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:34:13 INFO 139771206125376] Saved checkpoint to \"/opt/ml/model/state_6c0f4ad9-c32e-43be-bea7-184695982c30-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1717680852.9763772, \"EndTime\": 1717680853.15014, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 173.3877658843994, \"count\": 1, \"min\": 173.3877658843994, \"max\": 173.3877658843994}}}\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:34:16 INFO 139771206125376] Epoch[30] Batch[0] avg_epoch_loss=2.649014\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:34:16 INFO 139771206125376] #quality_metric: host=algo-1, epoch=30, batch=0 train loss <loss>=2.6490135192871094\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:34:25 INFO 139771206125376] Epoch[30] Batch[5] avg_epoch_loss=2.644094\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:34:25 INFO 139771206125376] #quality_metric: host=algo-1, epoch=30, batch=5 train loss <loss>=2.6440943479537964\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:34:25 INFO 139771206125376] Epoch[30] Batch [5]#011Speed: 37.80 samples/sec#011loss=2.644094\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:34:33 INFO 139771206125376] Epoch[30] Batch[10] avg_epoch_loss=2.667309\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:34:33 INFO 139771206125376] #quality_metric: host=algo-1, epoch=30, batch=10 train loss <loss>=2.695167636871338\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:34:33 INFO 139771206125376] Epoch[30] Batch [10]#011Speed: 37.07 samples/sec#011loss=2.695168\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:34:33 INFO 139771206125376] processed a total of 664 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1717680853.150206, \"EndTime\": 1717680873.6475048, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 20497.24006652832, \"count\": 1, \"min\": 20497.24006652832, \"max\": 20497.24006652832}}}\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:34:33 INFO 139771206125376] #throughput_metric: host=algo-1, train throughput=32.394457510189824 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:34:33 INFO 139771206125376] #progress_metric: host=algo-1, completed 7.75 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:34:33 INFO 139771206125376] #quality_metric: host=algo-1, epoch=30, train loss <loss>=2.6673094792799517\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:34:33 INFO 139771206125376] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:34:37 INFO 139771206125376] Epoch[31] Batch[0] avg_epoch_loss=2.661906\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:34:37 INFO 139771206125376] #quality_metric: host=algo-1, epoch=31, batch=0 train loss <loss>=2.6619057655334473\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[06/06/2024 13:34:45 INFO 139771206125376] Epoch[31] Batch[5] avg_epoch_loss=2.643259\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:34:45 INFO 139771206125376] #quality_metric: host=algo-1, epoch=31, batch=5 train loss <loss>=2.643259366353353\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:34:45 INFO 139771206125376] Epoch[31] Batch [5]#011Speed: 37.70 samples/sec#011loss=2.643259\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:34:54 INFO 139771206125376] Epoch[31] Batch[10] avg_epoch_loss=2.620497\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:34:54 INFO 139771206125376] #quality_metric: host=algo-1, epoch=31, batch=10 train loss <loss>=2.5931821346282957\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:34:54 INFO 139771206125376] Epoch[31] Batch [10]#011Speed: 37.16 samples/sec#011loss=2.593182\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:34:54 INFO 139771206125376] processed a total of 653 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1717680873.6475687, \"EndTime\": 1717680894.1842113, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 20536.36622428894, \"count\": 1, \"min\": 20536.36622428894, \"max\": 20536.36622428894}}}\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:34:54 INFO 139771206125376] #throughput_metric: host=algo-1, train throughput=31.797089358548686 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:34:54 INFO 139771206125376] #progress_metric: host=algo-1, completed 8.0 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:34:54 INFO 139771206125376] #quality_metric: host=algo-1, epoch=31, train loss <loss>=2.620496988296509\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:34:54 INFO 139771206125376] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:34:57 INFO 139771206125376] Epoch[32] Batch[0] avg_epoch_loss=2.691751\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:34:57 INFO 139771206125376] #quality_metric: host=algo-1, epoch=32, batch=0 train loss <loss>=2.691751003265381\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:35:06 INFO 139771206125376] Epoch[32] Batch[5] avg_epoch_loss=2.669792\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:35:06 INFO 139771206125376] #quality_metric: host=algo-1, epoch=32, batch=5 train loss <loss>=2.6697920163472495\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:35:06 INFO 139771206125376] Epoch[32] Batch [5]#011Speed: 35.17 samples/sec#011loss=2.669792\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:35:13 INFO 139771206125376] processed a total of 625 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1717680894.1842854, \"EndTime\": 1717680913.5936117, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 19408.889293670654, \"count\": 1, \"min\": 19408.889293670654, \"max\": 19408.889293670654}}}\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:35:13 INFO 139771206125376] #throughput_metric: host=algo-1, train throughput=32.201576285732486 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:35:13 INFO 139771206125376] #progress_metric: host=algo-1, completed 8.25 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:35:13 INFO 139771206125376] #quality_metric: host=algo-1, epoch=32, train loss <loss>=2.66467125415802\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:35:13 INFO 139771206125376] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:35:16 INFO 139771206125376] Epoch[33] Batch[0] avg_epoch_loss=2.522528\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:35:16 INFO 139771206125376] #quality_metric: host=algo-1, epoch=33, batch=0 train loss <loss>=2.5225276947021484\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:35:25 INFO 139771206125376] Epoch[33] Batch[5] avg_epoch_loss=2.623245\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:35:25 INFO 139771206125376] #quality_metric: host=algo-1, epoch=33, batch=5 train loss <loss>=2.6232453187306723\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:35:25 INFO 139771206125376] Epoch[33] Batch [5]#011Speed: 37.81 samples/sec#011loss=2.623245\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:35:34 INFO 139771206125376] Epoch[33] Batch[10] avg_epoch_loss=2.614894\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:35:34 INFO 139771206125376] #quality_metric: host=algo-1, epoch=33, batch=10 train loss <loss>=2.604872131347656\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:35:34 INFO 139771206125376] Epoch[33] Batch [10]#011Speed: 36.98 samples/sec#011loss=2.604872\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:35:34 INFO 139771206125376] processed a total of 699 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1717680913.5936794, \"EndTime\": 1717680934.0256474, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 20431.297063827515, \"count\": 1, \"min\": 20431.297063827515, \"max\": 20431.297063827515}}}\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:35:34 INFO 139771206125376] #throughput_metric: host=algo-1, train throughput=34.21199735692458 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:35:34 INFO 139771206125376] #progress_metric: host=algo-1, completed 8.5 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:35:34 INFO 139771206125376] #quality_metric: host=algo-1, epoch=33, train loss <loss>=2.6148938699202104\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:35:34 INFO 139771206125376] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:35:37 INFO 139771206125376] Epoch[34] Batch[0] avg_epoch_loss=2.750334\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:35:37 INFO 139771206125376] #quality_metric: host=algo-1, epoch=34, batch=0 train loss <loss>=2.750333786010742\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:35:45 INFO 139771206125376] Epoch[34] Batch[5] avg_epoch_loss=2.630178\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:35:45 INFO 139771206125376] #quality_metric: host=algo-1, epoch=34, batch=5 train loss <loss>=2.6301783323287964\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:35:45 INFO 139771206125376] Epoch[34] Batch [5]#011Speed: 37.83 samples/sec#011loss=2.630178\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:35:52 INFO 139771206125376] processed a total of 609 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1717680934.0257509, \"EndTime\": 1717680952.8151803, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 18788.991928100586, \"count\": 1, \"min\": 18788.991928100586, \"max\": 18788.991928100586}}}\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:35:52 INFO 139771206125376] #throughput_metric: host=algo-1, train throughput=32.41241519824405 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:35:52 INFO 139771206125376] #progress_metric: host=algo-1, completed 8.75 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:35:52 INFO 139771206125376] #quality_metric: host=algo-1, epoch=34, train loss <loss>=2.6062750816345215\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:35:52 INFO 139771206125376] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:35:56 INFO 139771206125376] Epoch[35] Batch[0] avg_epoch_loss=2.527332\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:35:56 INFO 139771206125376] #quality_metric: host=algo-1, epoch=35, batch=0 train loss <loss>=2.527332305908203\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:36:04 INFO 139771206125376] Epoch[35] Batch[5] avg_epoch_loss=2.606231\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:36:04 INFO 139771206125376] #quality_metric: host=algo-1, epoch=35, batch=5 train loss <loss>=2.606231371561686\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:36:04 INFO 139771206125376] Epoch[35] Batch [5]#011Speed: 37.34 samples/sec#011loss=2.606231\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:36:13 INFO 139771206125376] Epoch[35] Batch[10] avg_epoch_loss=2.554379\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:36:13 INFO 139771206125376] #quality_metric: host=algo-1, epoch=35, batch=10 train loss <loss>=2.492155647277832\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:36:13 INFO 139771206125376] Epoch[35] Batch [10]#011Speed: 37.31 samples/sec#011loss=2.492156\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:36:13 INFO 139771206125376] processed a total of 647 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1717680952.8152509, \"EndTime\": 1717680973.3837938, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 20568.081617355347, \"count\": 1, \"min\": 20568.081617355347, \"max\": 20568.081617355347}}}\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:36:13 INFO 139771206125376] #throughput_metric: host=algo-1, train throughput=31.45633910540566 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:36:13 INFO 139771206125376] #progress_metric: host=algo-1, completed 9.0 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:36:13 INFO 139771206125376] #quality_metric: host=algo-1, epoch=35, train loss <loss>=2.55437876961448\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:36:13 INFO 139771206125376] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:36:13 INFO 139771206125376] Saved checkpoint to \"/opt/ml/model/state_b3b3fa1e-3efa-4a1a-b221-7e34d60bf890-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1717680973.383873, \"EndTime\": 1717680973.5529556, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 168.65968704223633, \"count\": 1, \"min\": 168.65968704223633, \"max\": 168.65968704223633}}}\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:36:17 INFO 139771206125376] Epoch[36] Batch[0] avg_epoch_loss=2.619428\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:36:17 INFO 139771206125376] #quality_metric: host=algo-1, epoch=36, batch=0 train loss <loss>=2.6194279193878174\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[06/06/2024 13:36:25 INFO 139771206125376] Epoch[36] Batch[5] avg_epoch_loss=2.637874\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:36:25 INFO 139771206125376] #quality_metric: host=algo-1, epoch=36, batch=5 train loss <loss>=2.637873967488607\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:36:25 INFO 139771206125376] Epoch[36] Batch [5]#011Speed: 37.73 samples/sec#011loss=2.637874\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:36:32 INFO 139771206125376] processed a total of 616 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1717680973.5530176, \"EndTime\": 1717680992.311981, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 18758.907079696655, \"count\": 1, \"min\": 18758.907079696655, \"max\": 18758.907079696655}}}\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:36:32 INFO 139771206125376] #throughput_metric: host=algo-1, train throughput=32.83756409396397 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:36:32 INFO 139771206125376] #progress_metric: host=algo-1, completed 9.25 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:36:32 INFO 139771206125376] #quality_metric: host=algo-1, epoch=36, train loss <loss>=2.656950807571411\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:36:32 INFO 139771206125376] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:36:35 INFO 139771206125376] Epoch[37] Batch[0] avg_epoch_loss=2.620890\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:36:35 INFO 139771206125376] #quality_metric: host=algo-1, epoch=37, batch=0 train loss <loss>=2.6208901405334473\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:36:44 INFO 139771206125376] Epoch[37] Batch[5] avg_epoch_loss=2.629718\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:36:44 INFO 139771206125376] #quality_metric: host=algo-1, epoch=37, batch=5 train loss <loss>=2.6297181049982705\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:36:44 INFO 139771206125376] Epoch[37] Batch [5]#011Speed: 37.72 samples/sec#011loss=2.629718\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:36:52 INFO 139771206125376] Epoch[37] Batch[10] avg_epoch_loss=2.694681\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:36:52 INFO 139771206125376] #quality_metric: host=algo-1, epoch=37, batch=10 train loss <loss>=2.772637414932251\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:36:52 INFO 139771206125376] Epoch[37] Batch [10]#011Speed: 37.19 samples/sec#011loss=2.772637\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:36:52 INFO 139771206125376] processed a total of 653 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1717680992.312046, \"EndTime\": 1717681012.8320808, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 20519.60587501526, \"count\": 1, \"min\": 20519.60587501526, \"max\": 20519.60587501526}}}\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:36:52 INFO 139771206125376] #throughput_metric: host=algo-1, train throughput=31.823069122791306 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:36:52 INFO 139771206125376] #progress_metric: host=algo-1, completed 9.5 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:36:52 INFO 139771206125376] #quality_metric: host=algo-1, epoch=37, train loss <loss>=2.6946814276955346\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:36:52 INFO 139771206125376] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:36:56 INFO 139771206125376] Epoch[38] Batch[0] avg_epoch_loss=2.625094\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:36:56 INFO 139771206125376] #quality_metric: host=algo-1, epoch=38, batch=0 train loss <loss>=2.625094175338745\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:37:04 INFO 139771206125376] Epoch[38] Batch[5] avg_epoch_loss=2.615473\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:37:04 INFO 139771206125376] #quality_metric: host=algo-1, epoch=38, batch=5 train loss <loss>=2.6154728333155313\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:37:04 INFO 139771206125376] Epoch[38] Batch [5]#011Speed: 37.46 samples/sec#011loss=2.615473\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:37:11 INFO 139771206125376] processed a total of 616 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1717681012.832147, \"EndTime\": 1717681031.703052, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 18870.48101425171, \"count\": 1, \"min\": 18870.48101425171, \"max\": 18870.48101425171}}}\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:37:11 INFO 139771206125376] #throughput_metric: host=algo-1, train throughput=32.64339882848248 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:37:11 INFO 139771206125376] #progress_metric: host=algo-1, completed 9.75 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:37:11 INFO 139771206125376] #quality_metric: host=algo-1, epoch=38, train loss <loss>=2.6375123500823974\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:37:11 INFO 139771206125376] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:37:15 INFO 139771206125376] Epoch[39] Batch[0] avg_epoch_loss=2.539577\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:37:15 INFO 139771206125376] #quality_metric: host=algo-1, epoch=39, batch=0 train loss <loss>=2.539576530456543\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:37:23 INFO 139771206125376] Epoch[39] Batch[5] avg_epoch_loss=2.620087\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:37:23 INFO 139771206125376] #quality_metric: host=algo-1, epoch=39, batch=5 train loss <loss>=2.620086948076884\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:37:23 INFO 139771206125376] Epoch[39] Batch [5]#011Speed: 37.90 samples/sec#011loss=2.620087\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:37:30 INFO 139771206125376] processed a total of 618 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1717681031.703121, \"EndTime\": 1717681050.4176576, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 18714.06388282776, \"count\": 1, \"min\": 18714.06388282776, \"max\": 18714.06388282776}}}\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:37:30 INFO 139771206125376] #throughput_metric: host=algo-1, train throughput=33.02312139678301 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:37:30 INFO 139771206125376] #progress_metric: host=algo-1, completed 10.0 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:37:30 INFO 139771206125376] #quality_metric: host=algo-1, epoch=39, train loss <loss>=2.5934444665908813\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:37:30 INFO 139771206125376] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:37:33 INFO 139771206125376] Epoch[40] Batch[0] avg_epoch_loss=2.802342\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:37:33 INFO 139771206125376] #quality_metric: host=algo-1, epoch=40, batch=0 train loss <loss>=2.802341938018799\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:37:42 INFO 139771206125376] Epoch[40] Batch[5] avg_epoch_loss=2.599423\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:37:42 INFO 139771206125376] #quality_metric: host=algo-1, epoch=40, batch=5 train loss <loss>=2.5994232098261514\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:37:42 INFO 139771206125376] Epoch[40] Batch [5]#011Speed: 37.76 samples/sec#011loss=2.599423\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:37:50 INFO 139771206125376] Epoch[40] Batch[10] avg_epoch_loss=2.570202\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:37:50 INFO 139771206125376] #quality_metric: host=algo-1, epoch=40, batch=10 train loss <loss>=2.535136079788208\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:37:50 INFO 139771206125376] Epoch[40] Batch [10]#011Speed: 37.23 samples/sec#011loss=2.535136\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:37:50 INFO 139771206125376] processed a total of 644 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1717681050.4177234, \"EndTime\": 1717681070.9381504, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 20519.94824409485, \"count\": 1, \"min\": 20519.94824409485, \"max\": 20519.94824409485}}}\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:37:50 INFO 139771206125376] #throughput_metric: host=algo-1, train throughput=31.383943424005462 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:37:50 INFO 139771206125376] #progress_metric: host=algo-1, completed 10.25 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:37:50 INFO 139771206125376] #quality_metric: host=algo-1, epoch=40, train loss <loss>=2.570201787081632\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:37:50 INFO 139771206125376] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:37:54 INFO 139771206125376] Epoch[41] Batch[0] avg_epoch_loss=2.644882\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:37:54 INFO 139771206125376] #quality_metric: host=algo-1, epoch=41, batch=0 train loss <loss>=2.6448822021484375\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:38:02 INFO 139771206125376] Epoch[41] Batch[5] avg_epoch_loss=2.632987\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:38:02 INFO 139771206125376] #quality_metric: host=algo-1, epoch=41, batch=5 train loss <loss>=2.632987101872762\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:38:02 INFO 139771206125376] Epoch[41] Batch [5]#011Speed: 37.52 samples/sec#011loss=2.632987\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:38:09 INFO 139771206125376] processed a total of 587 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1717681070.9382198, \"EndTime\": 1717681089.7447631, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 18806.148290634155, \"count\": 1, \"min\": 18806.148290634155, \"max\": 18806.148290634155}}}\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:38:09 INFO 139771206125376] #throughput_metric: host=algo-1, train throughput=31.213027029925062 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:38:09 INFO 139771206125376] #progress_metric: host=algo-1, completed 10.5 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:38:09 INFO 139771206125376] #quality_metric: host=algo-1, epoch=41, train loss <loss>=2.6908830881118773\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:38:09 INFO 139771206125376] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:38:13 INFO 139771206125376] Epoch[42] Batch[0] avg_epoch_loss=2.494677\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:38:13 INFO 139771206125376] #quality_metric: host=algo-1, epoch=42, batch=0 train loss <loss>=2.4946770668029785\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[06/06/2024 13:38:21 INFO 139771206125376] Epoch[42] Batch[5] avg_epoch_loss=2.610039\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:38:21 INFO 139771206125376] #quality_metric: host=algo-1, epoch=42, batch=5 train loss <loss>=2.6100385586420694\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:38:21 INFO 139771206125376] Epoch[42] Batch [5]#011Speed: 37.79 samples/sec#011loss=2.610039\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:38:28 INFO 139771206125376] processed a total of 621 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1717681089.744831, \"EndTime\": 1717681108.52467, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 18779.327154159546, \"count\": 1, \"min\": 18779.327154159546, \"max\": 18779.327154159546}}}\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:38:28 INFO 139771206125376] #throughput_metric: host=algo-1, train throughput=33.0680964699801 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:38:28 INFO 139771206125376] #progress_metric: host=algo-1, completed 10.75 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:38:28 INFO 139771206125376] #quality_metric: host=algo-1, epoch=42, train loss <loss>=2.612620735168457\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:38:28 INFO 139771206125376] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:38:31 INFO 139771206125376] Epoch[43] Batch[0] avg_epoch_loss=2.657429\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:38:31 INFO 139771206125376] #quality_metric: host=algo-1, epoch=43, batch=0 train loss <loss>=2.6574289798736572\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:38:40 INFO 139771206125376] Epoch[43] Batch[5] avg_epoch_loss=2.624637\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:38:40 INFO 139771206125376] #quality_metric: host=algo-1, epoch=43, batch=5 train loss <loss>=2.6246374448140464\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:38:40 INFO 139771206125376] Epoch[43] Batch [5]#011Speed: 37.79 samples/sec#011loss=2.624637\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:38:47 INFO 139771206125376] processed a total of 629 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1717681108.52474, \"EndTime\": 1717681127.265788, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 18740.511894226074, \"count\": 1, \"min\": 18740.511894226074, \"max\": 18740.511894226074}}}\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:38:47 INFO 139771206125376] #throughput_metric: host=algo-1, train throughput=33.563440448760446 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:38:47 INFO 139771206125376] #progress_metric: host=algo-1, completed 11.0 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:38:47 INFO 139771206125376] #quality_metric: host=algo-1, epoch=43, train loss <loss>=2.616067099571228\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:38:47 INFO 139771206125376] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:38:50 INFO 139771206125376] Epoch[44] Batch[0] avg_epoch_loss=2.655684\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:38:50 INFO 139771206125376] #quality_metric: host=algo-1, epoch=44, batch=0 train loss <loss>=2.655683755874634\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:38:59 INFO 139771206125376] Epoch[44] Batch[5] avg_epoch_loss=2.626422\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:38:59 INFO 139771206125376] #quality_metric: host=algo-1, epoch=44, batch=5 train loss <loss>=2.62642240524292\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:38:59 INFO 139771206125376] Epoch[44] Batch [5]#011Speed: 37.49 samples/sec#011loss=2.626422\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:39:06 INFO 139771206125376] processed a total of 626 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1717681127.265861, \"EndTime\": 1717681146.2728734, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 19006.64973258972, \"count\": 1, \"min\": 19006.64973258972, \"max\": 19006.64973258972}}}\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:39:06 INFO 139771206125376] #throughput_metric: host=algo-1, train throughput=32.93562154805961 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:39:06 INFO 139771206125376] #progress_metric: host=algo-1, completed 11.25 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:39:06 INFO 139771206125376] #quality_metric: host=algo-1, epoch=44, train loss <loss>=2.6128239393234254\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:39:06 INFO 139771206125376] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:39:09 INFO 139771206125376] Epoch[45] Batch[0] avg_epoch_loss=2.678469\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:39:09 INFO 139771206125376] #quality_metric: host=algo-1, epoch=45, batch=0 train loss <loss>=2.67846941947937\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:39:18 INFO 139771206125376] Epoch[45] Batch[5] avg_epoch_loss=2.565924\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:39:18 INFO 139771206125376] #quality_metric: host=algo-1, epoch=45, batch=5 train loss <loss>=2.565924286842346\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:39:18 INFO 139771206125376] Epoch[45] Batch [5]#011Speed: 37.61 samples/sec#011loss=2.565924\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:39:25 INFO 139771206125376] processed a total of 638 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1717681146.2729526, \"EndTime\": 1717681165.114888, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 18841.429710388184, \"count\": 1, \"min\": 18841.429710388184, \"max\": 18841.429710388184}}}\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:39:25 INFO 139771206125376] #throughput_metric: host=algo-1, train throughput=33.86135428741764 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:39:25 INFO 139771206125376] #progress_metric: host=algo-1, completed 11.5 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:39:25 INFO 139771206125376] #quality_metric: host=algo-1, epoch=45, train loss <loss>=2.5871450185775755\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:39:25 INFO 139771206125376] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:39:25 INFO 139771206125376] Loading parameters from best epoch (35)\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1717681165.1149545, \"EndTime\": 1717681165.1948721, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.deserialize.time\": {\"sum\": 79.24342155456543, \"count\": 1, \"min\": 79.24342155456543, \"max\": 79.24342155456543}}}\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:39:25 INFO 139771206125376] stopping training now\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:39:25 INFO 139771206125376] #progress_metric: host=algo-1, completed 100 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:39:25 INFO 139771206125376] Final loss: 2.55437876961448 (occurred at epoch 35)\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:39:25 INFO 139771206125376] #quality_metric: host=algo-1, train final_loss <loss>=2.55437876961448\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:39:25 WARNING 139771206125376] You are using large values for `context_length` and/or `prediction_length`. The following step may take some time. If the step crashes, use an instance with more memory or reduce these two parameters.\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:39:25 INFO 139771206125376] Worker algo-1 finished training.\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:39:25 WARNING 139771206125376] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:39:25 INFO 139771206125376] All workers finished. Serializing model for prediction.\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1717681165.1949425, \"EndTime\": 1717681180.2080708, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"get_graph.time\": {\"sum\": 15012.388706207275, \"count\": 1, \"min\": 15012.388706207275, \"max\": 15012.388706207275}}}\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:39:41 INFO 139771206125376] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1717681180.2081475, \"EndTime\": 1717681181.0514777, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"finalize.time\": {\"sum\": 15855.84044456482, \"count\": 1, \"min\": 15855.84044456482, \"max\": 15855.84044456482}}}\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:39:41 INFO 139771206125376] Serializing to /opt/ml/model/model_algo-1\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:39:41 INFO 139771206125376] Saved checkpoint to \"/opt/ml/model/model_algo-1-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1717681181.0515342, \"EndTime\": 1717681181.1708236, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"model.serialize.time\": {\"sum\": 119.25029754638672, \"count\": 1, \"min\": 119.25029754638672, \"max\": 119.25029754638672}}}\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:39:41 INFO 139771206125376] Successfully serialized the model for prediction.\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:39:41 INFO 139771206125376] #memory_usage::<batchbuffer> = 203.9306640625 mb\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:39:41 INFO 139771206125376] Evaluating model accuracy on testset using 100 samples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1717681181.1708772, \"EndTime\": 1717681181.1765864, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"model.bind.time\": {\"sum\": 0.03886222839355469, \"count\": 1, \"min\": 0.03886222839355469, \"max\": 0.03886222839355469}}}\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1717681181.1766357, \"EndTime\": 1717681186.7815335, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"model.score.time\": {\"sum\": 5604.990005493164, \"count\": 1, \"min\": 5604.990005493164, \"max\": 5604.990005493164}}}\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:39:46 INFO 139771206125376] #test_score (algo-1, RMSE): 45.362605199887206\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:39:46 INFO 139771206125376] #test_score (algo-1, mean_absolute_QuantileLoss): 17729.794600607784\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:39:46 INFO 139771206125376] #test_score (algo-1, mean_wQuantileLoss): 0.14729413143314604\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:39:46 INFO 139771206125376] #test_score (algo-1, wQuantileLoss[0.1]): 0.08567792510594147\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:39:46 INFO 139771206125376] #test_score (algo-1, wQuantileLoss[0.2]): 0.13062777734183093\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:39:46 INFO 139771206125376] #test_score (algo-1, wQuantileLoss[0.3]): 0.16279095860444953\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:39:46 INFO 139771206125376] #test_score (algo-1, wQuantileLoss[0.4]): 0.179174375629429\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:39:46 INFO 139771206125376] #test_score (algo-1, wQuantileLoss[0.5]): 0.18669416645780557\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:39:46 INFO 139771206125376] #test_score (algo-1, wQuantileLoss[0.6]): 0.1821908811134358\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:39:46 INFO 139771206125376] #test_score (algo-1, wQuantileLoss[0.7]): 0.16698560372480248\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:39:46 INFO 139771206125376] #test_score (algo-1, wQuantileLoss[0.8]): 0.13887659727016774\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:39:46 INFO 139771206125376] #test_score (algo-1, wQuantileLoss[0.9]): 0.09262889765045158\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:39:46 INFO 139771206125376] #quality_metric: host=algo-1, test RMSE <loss>=45.362605199887206\u001b[0m\n",
      "\u001b[34m[06/06/2024 13:39:46 INFO 139771206125376] #quality_metric: host=algo-1, test mean_wQuantileLoss <loss>=0.14729413143314604\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1717681186.7816026, \"EndTime\": 1717681187.0592046, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"setuptime\": {\"sum\": 4.187345504760742, \"count\": 1, \"min\": 4.187345504760742, \"max\": 4.187345504760742}, \"totaltime\": {\"sum\": 952221.4472293854, \"count\": 1, \"min\": 952221.4472293854, \"max\": 952221.4472293854}}}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2024-06-06 13:39:55 Uploading - Uploading generated training model\n",
      "2024-06-06 13:40:08 Completed - Training job completed\n",
      "Training seconds: 1112\n",
      "Billable seconds: 500\n",
      "Managed Spot Training savings: 55.0%\n"
     ]
    }
   ],
   "source": [
    "# fitting the model\n",
    "\n",
    "estimator.fit(inputs=data_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f1b3efdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = estimator.latest_training_job.name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841a6bc6",
   "metadata": {},
   "source": [
    "#### Create endpoint using jobname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9f17a6f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job name: deepar-biketrain-no-categories-2024-06-06-13-20-47-915\n"
     ]
    }
   ],
   "source": [
    "print('job name: {0}'.format(job_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8f20180f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: deepar-biketrain-no-categories-2024-06-06-13-20-47-915\n",
      "INFO:sagemaker:Creating endpoint-config with name deepar-biketrain-no-categories-2024-06-06-13-20-47-915\n",
      "INFO:sagemaker:Creating endpoint with name deepar-biketrain-no-categories-2024-06-06-13-20-47-915\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------!"
     ]
    }
   ],
   "source": [
    "# create an endpoint for real-time predictions\n",
    "\n",
    "endpoint_name = sess.endpoint_from_job(\n",
    "    job_name = job_name,\n",
    "    initial_instance_count = 1,\n",
    "    instance_type = 'ml.m5.large' if use_spot_instances else 'ml.m5.xlarge',\n",
    "    image_uri = container, \n",
    "    role = role\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed8e7a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p310",
   "language": "python",
   "name": "conda_tensorflow2_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
