{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f573070",
   "metadata": {},
   "source": [
    "## DeepAR Model - Predict Bike Rental with Dynamic Features\n",
    "\n",
    "Note: This dataset is not a true timeseries as there a lot of gaps\n",
    "\n",
    "We have data only for first 20 days of each month and model needs to predict the rentals for \n",
    "the remaining days of the month. The dataset consists of two years data. DeepAR will shine with true multiple-timeseries dataset like the electricity example given below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d0a97e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5aa7521",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_categories =False\n",
    "\n",
    "base_job_name = 'deepar-biketrain-with-dynamic-feat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28a2251a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import s3 bucket name as environment variable\n",
    "\n",
    "import os\n",
    "env_vars = !cat ./.env\n",
    "for var in env_vars:\n",
    "    key, value = var.split('=')\n",
    "    os.environ[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4436bd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify your bucket name and dataset path in that\n",
    "\n",
    "bucket = os.environ['BUCKET_NAME']\n",
    "prefix = 'deepar/bikerental'\n",
    "\n",
    "s3_data_path = \"{}/{}/data_dynamic\".format(bucket, prefix)\n",
    "s3_output_path = \"{}/{}/output\".format(bucket,prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65ab6842",
   "metadata": {},
   "outputs": [],
   "source": [
    "#s3_data_path, s3_output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a80a663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In S3 filename is keyname\n",
    "\n",
    "def write_to_s3(filename, bucket, key):\n",
    "    with open(filename,'rb') as f:\n",
    "        return boto3.Session().resource('s3').Bucket(bucket).Object(key).upload_fileobj(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5c5914f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload one or more files to s3\n",
    "\n",
    "write_to_s3('train_dynamic_feat.json', bucket, 'deepar/bikerental/data_dynamic/train/train_dynamic_feat.json')\n",
    "write_to_s3('test_dynamic_feat.json', bucket, 'deepar/bikerental/data_dynamic/test/test_dynamic_feat.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1870fe7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use spot instance\n",
    "\n",
    "use_spot_instances = True\n",
    "max_run = 3600\n",
    "max_wait = 3600 if use_spot_instances else None\n",
    "\n",
    "job_name = base_job_name\n",
    "\n",
    "checkpoint_s3_uri = None\n",
    "\n",
    "if use_spot_instances:\n",
    "    checkpoint_s3_uri = f's3://{bucket}/{prefix}/checkpoints/{job_name}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b17c6da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(checkpoint_s3_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a7225aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish a Session with AWS\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eb305c29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sagemaker.session.Session at 0x7f4db5596230>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a5cb162a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "26c2951c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use deepar container\n",
    "\n",
    "container = sagemaker.image_uris.retrieve(\"forecasting-deepar\",sess.boto_region_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "890c6845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "522234722520.dkr.ecr.us-east-1.amazonaws.com/forecasting-deepar:1\n"
     ]
    }
   ],
   "source": [
    "print(container)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a3d15203",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = 'H'   # Timeseries contains hourly data\n",
    "\n",
    "# Prediction length is tweleve days\n",
    "\n",
    "prediction_length = 12*24\n",
    "\n",
    "# context length is how far in the past it should look for prediction\n",
    "# AWS recommends context length same as prediction length\n",
    "\n",
    "context_length = prediction_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "88d1af18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "288\n"
     ]
    }
   ],
   "source": [
    "print(context_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b7075ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#configure the training job\n",
    "\n",
    "estimator = sagemaker.estimator.Estimator(\n",
    "    container,\n",
    "    role,\n",
    "    instance_count=1,\n",
    "    instance_type = 'ml.m5.xlarge',\n",
    "    outut_path='s3://'+s3_output_path,\n",
    "    sagemaker_session=sess,\n",
    "    base_job_name=base_job_name,\n",
    "    use_spot_instances=use_spot_instances,\n",
    "    max_run=max_run,\n",
    "    max_wait=max_wait,\n",
    "    checkpoint_s3_uri=checkpoint_s3_uri\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a1686b11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('H', 288, 288)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq, context_length, prediction_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1314d8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.aws.amazon.com/sagemaker/latest/dg/deepar_hyperparameters.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b15962f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"time_freq\": freq,\n",
    "    \"epochs\": \"400\",\n",
    "    \"early_stopping_patience\": \"10\",\n",
    "    \"mini_batch_size\":\"64\",\n",
    "    \"learning_rate\": \"5E-4\",\n",
    "    \"context_length\": str(context_length),\n",
    "    \"prediction_length\": str(prediction_length),\n",
    "    \"cardinality\" : \"auto\" if with_categories else ''\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b0ec643d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'time_freq': 'H',\n",
       " 'epochs': '400',\n",
       " 'early_stopping_patience': '10',\n",
       " 'mini_batch_size': '64',\n",
       " 'learning_rate': '5E-4',\n",
       " 'context_length': '288',\n",
       " 'prediction_length': '288',\n",
       " 'cardinality': ''}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6a8e0ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.set_hyperparameters(**hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "507c8f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and test data path in s3 \n",
    "\n",
    "data_channels = {\n",
    "    \"train\": \"s3://{}/train/\".format(s3_data_path),\n",
    "    \"test\": \"s3://{}/train/\".format(s3_data_path)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "035aa89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c6933089",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: deepar-biketrain-with-dynamic-feat-2024-06-11-19-51-55-174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-11 19:51:55 Starting - Starting the training job...\n",
      "2024-06-11 19:52:12 Starting - Preparing the instances for training...\n",
      "2024-06-11 19:52:41 Downloading - Downloading input data...\n",
      "2024-06-11 19:53:02 Downloading - Downloading the training image..................\n",
      "2024-06-11 19:56:18 Training - Training image download completed. Training in progress..\u001b[34mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[34mRunning default environment configuration script\u001b[0m\n",
      "\u001b[34mRunning custom environment configuration script\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python3.8/site-packages/mxnet/model.py:97: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if num_device is 1 and 'dist' not in kvstore:\u001b[0m\n",
      "\u001b[34m[06/11/2024 19:56:30 INFO 140205716232000] Reading default configuration from /opt/amazon/lib/python3.8/site-packages/algorithm/resources/default-input.json: {'_kvstore': 'auto', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_tuning_objective_metric': '', 'cardinality': 'auto', 'dropout_rate': '0.10', 'early_stopping_patience': '', 'embedding_dimension': '10', 'learning_rate': '0.001', 'likelihood': 'student-t', 'mini_batch_size': '128', 'num_cells': '40', 'num_dynamic_feat': 'auto', 'num_eval_samples': '100', 'num_layers': '2', 'test_quantiles': '[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]'}\u001b[0m\n",
      "\u001b[34m[06/11/2024 19:56:30 INFO 140205716232000] Merging with provided configuration from /opt/ml/input/config/hyperparameters.json: {'context_length': '288', 'early_stopping_patience': '10', 'epochs': '400', 'learning_rate': '5E-4', 'mini_batch_size': '64', 'prediction_length': '288', 'time_freq': 'H'}\u001b[0m\n",
      "\u001b[34m[06/11/2024 19:56:30 INFO 140205716232000] Final configuration: {'_kvstore': 'auto', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_tuning_objective_metric': '', 'cardinality': 'auto', 'dropout_rate': '0.10', 'early_stopping_patience': '10', 'embedding_dimension': '10', 'learning_rate': '5E-4', 'likelihood': 'student-t', 'mini_batch_size': '64', 'num_cells': '40', 'num_dynamic_feat': 'auto', 'num_eval_samples': '100', 'num_layers': '2', 'test_quantiles': '[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', 'context_length': '288', 'epochs': '400', 'prediction_length': '288', 'time_freq': 'H'}\u001b[0m\n",
      "\u001b[34mProcess 7 is a worker.\u001b[0m\n",
      "\u001b[34m[06/11/2024 19:56:30 INFO 140205716232000] Detected entry point for worker worker\u001b[0m\n",
      "\u001b[34m[06/11/2024 19:56:30 INFO 140205716232000] Using early stopping with patience 10\u001b[0m\n",
      "\u001b[34m[06/11/2024 19:56:30 INFO 140205716232000] random_seed is None\u001b[0m\n",
      "\u001b[34m[06/11/2024 19:56:30 INFO 140205716232000] [cardinality=auto] `cat` field was NOT found in the file `/opt/ml/input/data/train/train_dynamic_feat.json` and will NOT be used for training.\u001b[0m\n",
      "\u001b[34m[06/11/2024 19:56:30 INFO 140205716232000] [num_dynamic_feat=auto] `dynamic_feat` field was found in the file `/opt/ml/input/data/train/train_dynamic_feat.json` and will be used for training.\u001b[0m\n",
      "\u001b[34m[06/11/2024 19:56:30 INFO 140205716232000] [num_dynamic_feat=auto] Inferred value of num_dynamic_feat=8 from dataset.\u001b[0m\n",
      "\u001b[34m[06/11/2024 19:56:30 INFO 140205716232000] Training set statistics:\u001b[0m\n",
      "\u001b[34m[06/11/2024 19:56:30 INFO 140205716232000] Integer time series\u001b[0m\n",
      "\u001b[34m[06/11/2024 19:56:30 INFO 140205716232000] number of time series: 3\u001b[0m\n",
      "\u001b[34m[06/11/2024 19:56:30 INFO 140205716232000] number of observations: 50904\u001b[0m\n",
      "\u001b[34m[06/11/2024 19:56:30 INFO 140205716232000] mean target length: 16968.0\u001b[0m\n",
      "\u001b[34m[06/11/2024 19:56:30 INFO 140205716232000] min/mean/max target: 0.0/79.57296086751532/977.0\u001b[0m\n",
      "\u001b[34m[06/11/2024 19:56:30 INFO 140205716232000] mean abs(target): 79.57296086751532\u001b[0m\n",
      "\u001b[34m[06/11/2024 19:56:30 INFO 140205716232000] contains missing values: yes (37.5%)\u001b[0m\n",
      "\u001b[34m[06/11/2024 19:56:30 INFO 140205716232000] Small number of time series. Doing 214 passes over dataset with prob 0.9968847352024922 per epoch.\u001b[0m\n",
      "\u001b[34m[06/11/2024 19:56:30 INFO 140205716232000] Test set statistics:\u001b[0m\n",
      "\u001b[34m[06/11/2024 19:56:30 INFO 140205716232000] Integer time series\u001b[0m\n",
      "\u001b[34m[06/11/2024 19:56:30 INFO 140205716232000] number of time series: 3\u001b[0m\n",
      "\u001b[34m[06/11/2024 19:56:30 INFO 140205716232000] number of observations: 50904\u001b[0m\n",
      "\u001b[34m[06/11/2024 19:56:30 INFO 140205716232000] mean target length: 16968.0\u001b[0m\n",
      "\u001b[34m[06/11/2024 19:56:30 INFO 140205716232000] min/mean/max target: 0.0/79.57296086751532/977.0\u001b[0m\n",
      "\u001b[34m[06/11/2024 19:56:30 INFO 140205716232000] mean abs(target): 79.57296086751532\u001b[0m\n",
      "\u001b[34m[06/11/2024 19:56:30 INFO 140205716232000] contains missing values: yes (37.5%)\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python3.8/site-packages/algorithm/core/date_feature_set.py:44: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)\n",
      "  return index.weekofyear / 51.0 - 0.5\u001b[0m\n",
      "\u001b[34m[06/11/2024 19:56:30 INFO 140205716232000] #memory_usage::<batchbuffer> = 385.8251953125 mb\u001b[0m\n",
      "\u001b[34m/opt/amazon/python3.8/lib/python3.8/subprocess.py:848: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used\n",
      "  self.stdout = io.open(c2pread, 'rb', bufsize)\u001b[0m\n",
      "\u001b[34m[06/11/2024 19:56:30 INFO 140205716232000] nvidia-smi: took 0.031 seconds to run.\u001b[0m\n",
      "\u001b[34m[06/11/2024 19:56:30 INFO 140205716232000] nvidia-smi identified 0 GPUs.\u001b[0m\n",
      "\u001b[34m[06/11/2024 19:56:30 INFO 140205716232000] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[06/11/2024 19:56:30 INFO 140205716232000] Create Store: local\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1718135790.517404, \"EndTime\": 1718135803.6353016, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"get_graph.time\": {\"sum\": 13114.896059036255, \"count\": 1, \"min\": 13114.896059036255, \"max\": 13114.896059036255}}}\u001b[0m\n",
      "\u001b[34m[06/11/2024 19:56:43 INFO 140205716232000] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[06/11/2024 19:56:46 INFO 140205716232000] #memory_usage::<model> = 279 mb\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1718135803.6353765, \"EndTime\": 1718135806.416356, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"initialize.time\": {\"sum\": 15898.810148239136, \"count\": 1, \"min\": 15898.810148239136, \"max\": 15898.810148239136}}}\u001b[0m\n",
      "\u001b[34m[19:56:58] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.3.x_Cuda_11.1.x.406.0/AL2_x86_64/generic-flavor/src/src/operator/nn/mkldnn/mkldnn_base.cc:74: Allocate 10240 bytes with malloc directly\u001b[0m\n",
      "\u001b[34m[06/11/2024 19:57:00 INFO 140205716232000] Epoch[0] Batch[0] avg_epoch_loss=4.118539\u001b[0m\n",
      "\u001b[34m[06/11/2024 19:57:00 INFO 140205716232000] #quality_metric: host=algo-1, epoch=0, batch=0 train loss <loss>=4.118538856506348\u001b[0m\n",
      "\u001b[34m[06/11/2024 19:57:11 INFO 140205716232000] Epoch[0] Batch[5] avg_epoch_loss=3.791855\u001b[0m\n",
      "\u001b[34m[06/11/2024 19:57:11 INFO 140205716232000] #quality_metric: host=algo-1, epoch=0, batch=5 train loss <loss>=3.7918551762898765\u001b[0m\n",
      "\u001b[34m[06/11/2024 19:57:11 INFO 140205716232000] Epoch[0] Batch [5]#011Speed: 28.29 samples/sec#011loss=3.791855\u001b[0m\n",
      "\u001b[34m[06/11/2024 19:57:20 INFO 140205716232000] processed a total of 589 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1718135806.4164295, \"EndTime\": 1718135840.0271971, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"epochs\": {\"sum\": 400.0, \"count\": 1, \"min\": 400, \"max\": 400}, \"update.time\": {\"sum\": 33610.69345474243, \"count\": 1, \"min\": 33610.69345474243, \"max\": 33610.69345474243}}}\u001b[0m\n",
      "\u001b[34m[06/11/2024 19:57:20 INFO 140205716232000] #throughput_metric: host=algo-1, train throughput=17.524128761522096 records/second\u001b[0m\n",
      "\u001b[34m[06/11/2024 19:57:20 INFO 140205716232000] #progress_metric: host=algo-1, completed 0.25 % of epochs\u001b[0m\n",
      "\u001b[34m[06/11/2024 19:57:20 INFO 140205716232000] #quality_metric: host=algo-1, epoch=0, train loss <loss>=3.7579805850982666\u001b[0m\n",
      "\u001b[34m[06/11/2024 19:57:20 INFO 140205716232000] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/11/2024 19:57:20 INFO 140205716232000] Saved checkpoint to \"/opt/ml/model/state_afea4efc-6cc7-4835-aaf7-73c102f142e9-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1718135840.0272727, \"EndTime\": 1718135840.2092338, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 181.5953254699707, \"count\": 1, \"min\": 181.5953254699707, \"max\": 181.5953254699707}}}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[06/11/2024 19:57:33 INFO 140205716232000] Epoch[1] Batch[0] avg_epoch_loss=3.643927\u001b[0m\n",
      "\u001b[34m[06/11/2024 19:57:33 INFO 140205716232000] #quality_metric: host=algo-1, epoch=1, batch=0 train loss <loss>=3.6439270973205566\u001b[0m\n",
      "\u001b[34m[06/11/2024 19:57:43 INFO 140205716232000] Epoch[1] Batch[5] avg_epoch_loss=3.555964\u001b[0m\n",
      "\u001b[34m[06/11/2024 19:57:43 INFO 140205716232000] #quality_metric: host=algo-1, epoch=1, batch=5 train loss <loss>=3.55596387386322\u001b[0m\n",
      "\u001b[34m[06/11/2024 19:57:43 INFO 140205716232000] Epoch[1] Batch [5]#011Speed: 31.22 samples/sec#011loss=3.555964\u001b[0m\n",
      "\u001b[34m[06/11/2024 19:57:51 INFO 140205716232000] processed a total of 639 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1718135840.2092967, \"EndTime\": 1718135871.8831902, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 31673.832654953003, \"count\": 1, \"min\": 31673.832654953003, \"max\": 31673.832654953003}}}\u001b[0m\n",
      "\u001b[34m[06/11/2024 19:57:51 INFO 140205716232000] #throughput_metric: host=algo-1, train throughput=20.174306394057528 records/second\u001b[0m\n",
      "\u001b[34m[06/11/2024 19:57:51 INFO 140205716232000] #progress_metric: host=algo-1, completed 0.5 % of epochs\u001b[0m\n",
      "\u001b[34m[06/11/2024 19:57:51 INFO 140205716232000] #quality_metric: host=algo-1, epoch=1, train loss <loss>=3.532480239868164\u001b[0m\n",
      "\u001b[34m[06/11/2024 19:57:51 INFO 140205716232000] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/11/2024 19:57:52 INFO 140205716232000] Saved checkpoint to \"/opt/ml/model/state_ad969099-7760-46c4-9276-3893b59d1f0b-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1718135871.8832617, \"EndTime\": 1718135872.0641994, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 180.4361343383789, \"count\": 1, \"min\": 180.4361343383789, \"max\": 180.4361343383789}}}\u001b[0m\n",
      "\u001b[34m[06/11/2024 19:58:05 INFO 140205716232000] Epoch[2] Batch[0] avg_epoch_loss=3.471387\u001b[0m\n",
      "\u001b[34m[06/11/2024 19:58:05 INFO 140205716232000] #quality_metric: host=algo-1, epoch=2, batch=0 train loss <loss>=3.4713873863220215\u001b[0m\n",
      "\u001b[34m[06/11/2024 19:58:15 INFO 140205716232000] Epoch[2] Batch[5] avg_epoch_loss=3.556535\u001b[0m\n",
      "\u001b[34m[06/11/2024 19:58:15 INFO 140205716232000] #quality_metric: host=algo-1, epoch=2, batch=5 train loss <loss>=3.5565346082051597\u001b[0m\n",
      "\u001b[34m[06/11/2024 19:58:15 INFO 140205716232000] Epoch[2] Batch [5]#011Speed: 31.47 samples/sec#011loss=3.556535\u001b[0m\n",
      "\u001b[34m[06/11/2024 19:58:23 INFO 140205716232000] processed a total of 626 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1718135872.064266, \"EndTime\": 1718135903.977917, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 31913.589239120483, \"count\": 1, \"min\": 31913.589239120483, \"max\": 31913.589239120483}}}\u001b[0m\n",
      "\u001b[34m[06/11/2024 19:58:23 INFO 140205716232000] #throughput_metric: host=algo-1, train throughput=19.615406520141384 records/second\u001b[0m\n",
      "\u001b[34m[06/11/2024 19:58:23 INFO 140205716232000] #progress_metric: host=algo-1, completed 0.75 % of epochs\u001b[0m\n",
      "\u001b[34m[06/11/2024 19:58:23 INFO 140205716232000] #quality_metric: host=algo-1, epoch=2, train loss <loss>=3.597026562690735\u001b[0m\n",
      "\u001b[34m[06/11/2024 19:58:23 INFO 140205716232000] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/11/2024 19:58:36 INFO 140205716232000] Epoch[3] Batch[0] avg_epoch_loss=3.482856\u001b[0m\n",
      "\u001b[34m[06/11/2024 19:58:36 INFO 140205716232000] #quality_metric: host=algo-1, epoch=3, batch=0 train loss <loss>=3.482855796813965\u001b[0m\n",
      "\u001b[34m[06/11/2024 19:58:46 INFO 140205716232000] Epoch[3] Batch[5] avg_epoch_loss=3.533607\u001b[0m\n",
      "\u001b[34m[06/11/2024 19:58:46 INFO 140205716232000] #quality_metric: host=algo-1, epoch=3, batch=5 train loss <loss>=3.533606926600138\u001b[0m\n",
      "\u001b[34m[06/11/2024 19:58:46 INFO 140205716232000] Epoch[3] Batch [5]#011Speed: 31.36 samples/sec#011loss=3.533607\u001b[0m\n",
      "\u001b[34m[06/11/2024 19:58:57 INFO 140205716232000] Epoch[3] Batch[10] avg_epoch_loss=3.538736\u001b[0m\n",
      "\u001b[34m[06/11/2024 19:58:57 INFO 140205716232000] #quality_metric: host=algo-1, epoch=3, batch=10 train loss <loss>=3.5448906898498533\u001b[0m\n",
      "\u001b[34m[06/11/2024 19:58:57 INFO 140205716232000] Epoch[3] Batch [10]#011Speed: 29.98 samples/sec#011loss=3.544891\u001b[0m\n",
      "\u001b[34m[06/11/2024 19:58:57 INFO 140205716232000] processed a total of 660 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1718135903.9779856, \"EndTime\": 1718135937.3993058, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 33420.902967453, \"count\": 1, \"min\": 33420.902967453, \"max\": 33420.902967453}}}\u001b[0m\n",
      "\u001b[34m[06/11/2024 19:58:57 INFO 140205716232000] #throughput_metric: host=algo-1, train throughput=19.748054703919426 records/second\u001b[0m\n",
      "\u001b[34m[06/11/2024 19:58:57 INFO 140205716232000] #progress_metric: host=algo-1, completed 1.0 % of epochs\u001b[0m\n",
      "\u001b[34m[06/11/2024 19:58:57 INFO 140205716232000] #quality_metric: host=algo-1, epoch=3, train loss <loss>=3.5387359098954634\u001b[0m\n",
      "\u001b[34m[06/11/2024 19:58:57 INFO 140205716232000] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/11/2024 19:59:10 INFO 140205716232000] Epoch[4] Batch[0] avg_epoch_loss=3.634784\u001b[0m\n",
      "\u001b[34m[06/11/2024 19:59:10 INFO 140205716232000] #quality_metric: host=algo-1, epoch=4, batch=0 train loss <loss>=3.6347837448120117\u001b[0m\n",
      "\u001b[34m[06/11/2024 19:59:20 INFO 140205716232000] Epoch[4] Batch[5] avg_epoch_loss=3.488431\u001b[0m\n",
      "\u001b[34m[06/11/2024 19:59:20 INFO 140205716232000] #quality_metric: host=algo-1, epoch=4, batch=5 train loss <loss>=3.4884312550226846\u001b[0m\n",
      "\u001b[34m[06/11/2024 19:59:20 INFO 140205716232000] Epoch[4] Batch [5]#011Speed: 31.52 samples/sec#011loss=3.488431\u001b[0m\n",
      "\n",
      "2024-06-11 20:00:06 Interrupted - Training job interrupted\n",
      "2024-06-11 20:01:08 Starting - Starting the training job\n",
      "2024-06-11 20:01:47 Starting - Preparing the instances for training\n",
      "2024-06-11 20:02:32 Downloading - Downloading the training image\n",
      "2024-06-11 20:07:37 Interrupted - Training job interrupted\n",
      "2024-06-11 20:08:08 Starting - Preparing the instances for training\n",
      "2024-06-11 20:08:35 Downloading - Downloading input data\n",
      "2024-06-11 20:08:55 Downloading - Downloading the training image\n",
      "2024-06-11 20:12:21 Training - Training image download completed. Training in progress.\u001b[32mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[32mRunning default environment configuration script\u001b[0m\n",
      "\u001b[32mRunning custom environment configuration script\u001b[0m\n",
      "\u001b[32m/opt/amazon/lib/python3.8/site-packages/mxnet/model.py:97: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if num_device is 1 and 'dist' not in kvstore:\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:12:34 INFO 139746470463296] Reading default configuration from /opt/amazon/lib/python3.8/site-packages/algorithm/resources/default-input.json: {'_kvstore': 'auto', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_tuning_objective_metric': '', 'cardinality': 'auto', 'dropout_rate': '0.10', 'early_stopping_patience': '', 'embedding_dimension': '10', 'learning_rate': '0.001', 'likelihood': 'student-t', 'mini_batch_size': '128', 'num_cells': '40', 'num_dynamic_feat': 'auto', 'num_eval_samples': '100', 'num_layers': '2', 'test_quantiles': '[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]'}\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:12:34 INFO 139746470463296] Merging with provided configuration from /opt/ml/input/config/hyperparameters.json: {'context_length': '288', 'early_stopping_patience': '10', 'epochs': '400', 'learning_rate': '5E-4', 'mini_batch_size': '64', 'prediction_length': '288', 'time_freq': 'H'}\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:12:34 INFO 139746470463296] Final configuration: {'_kvstore': 'auto', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_tuning_objective_metric': '', 'cardinality': 'auto', 'dropout_rate': '0.10', 'early_stopping_patience': '10', 'embedding_dimension': '10', 'learning_rate': '5E-4', 'likelihood': 'student-t', 'mini_batch_size': '64', 'num_cells': '40', 'num_dynamic_feat': 'auto', 'num_eval_samples': '100', 'num_layers': '2', 'test_quantiles': '[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', 'context_length': '288', 'epochs': '400', 'prediction_length': '288', 'time_freq': 'H'}\u001b[0m\n",
      "\u001b[32mProcess 7 is a worker.\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:12:34 INFO 139746470463296] Detected entry point for worker worker\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:12:34 INFO 139746470463296] Using early stopping with patience 10\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:12:34 INFO 139746470463296] random_seed is None\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:12:34 INFO 139746470463296] [cardinality=auto] `cat` field was NOT found in the file `/opt/ml/input/data/train/train_dynamic_feat.json` and will NOT be used for training.\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:12:34 INFO 139746470463296] [num_dynamic_feat=auto] `dynamic_feat` field was found in the file `/opt/ml/input/data/train/train_dynamic_feat.json` and will be used for training.\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:12:34 INFO 139746470463296] [num_dynamic_feat=auto] Inferred value of num_dynamic_feat=8 from dataset.\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:12:34 INFO 139746470463296] Training set statistics:\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:12:34 INFO 139746470463296] Integer time series\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:12:34 INFO 139746470463296] number of time series: 3\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:12:34 INFO 139746470463296] number of observations: 50904\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:12:34 INFO 139746470463296] mean target length: 16968.0\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:12:34 INFO 139746470463296] min/mean/max target: 0.0/79.57296086751532/977.0\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:12:34 INFO 139746470463296] mean abs(target): 79.57296086751532\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:12:34 INFO 139746470463296] contains missing values: yes (37.5%)\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:12:34 INFO 139746470463296] Small number of time series. Doing 214 passes over dataset with prob 0.9968847352024922 per epoch.\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:12:34 INFO 139746470463296] Test set statistics:\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:12:34 INFO 139746470463296] Integer time series\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:12:34 INFO 139746470463296] number of time series: 3\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:12:34 INFO 139746470463296] number of observations: 50904\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:12:34 INFO 139746470463296] mean target length: 16968.0\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:12:34 INFO 139746470463296] min/mean/max target: 0.0/79.57296086751532/977.0\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:12:34 INFO 139746470463296] mean abs(target): 79.57296086751532\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:12:34 INFO 139746470463296] contains missing values: yes (37.5%)\u001b[0m\n",
      "\u001b[32m/opt/amazon/lib/python3.8/site-packages/algorithm/core/date_feature_set.py:44: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)\n",
      "  return index.weekofyear / 51.0 - 0.5\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:12:34 INFO 139746470463296] #memory_usage::<batchbuffer> = 385.8251953125 mb\u001b[0m\n",
      "\u001b[32m/opt/amazon/python3.8/lib/python3.8/subprocess.py:848: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used\n",
      "  self.stdout = io.open(c2pread, 'rb', bufsize)\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:12:34 INFO 139746470463296] nvidia-smi: took 0.031 seconds to run.\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:12:34 INFO 139746470463296] nvidia-smi identified 0 GPUs.\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:12:34 INFO 139746470463296] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:12:34 INFO 139746470463296] Create Store: local\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m#metrics {\"StartTime\": 1718136754.7023, \"EndTime\": 1718136767.49206, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"get_graph.time\": {\"sum\": 12786.656141281128, \"count\": 1, \"min\": 12786.656141281128, \"max\": 12786.656141281128}}}\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:12:47 INFO 139746470463296] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:12:50 INFO 139746470463296] #memory_usage::<model> = 279 mb\u001b[0m\n",
      "\u001b[32m#metrics {\"StartTime\": 1718136767.492153, \"EndTime\": 1718136770.7370257, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"initialize.time\": {\"sum\": 16034.56997871399, \"count\": 1, \"min\": 16034.56997871399, \"max\": 16034.56997871399}}}\u001b[0m\n",
      "\u001b[32m[20:13:03] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.3.x_Cuda_11.1.x.406.0/AL2_x86_64/generic-flavor/src/src/operator/nn/mkldnn/mkldnn_base.cc:74: Allocate 10240 bytes with malloc directly\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:13:05 INFO 139746470463296] Epoch[0] Batch[0] avg_epoch_loss=3.946731\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:13:05 INFO 139746470463296] #quality_metric: host=algo-1, epoch=0, batch=0 train loss <loss>=3.946730852127075\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:13:16 INFO 139746470463296] Epoch[0] Batch[5] avg_epoch_loss=3.847812\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:13:16 INFO 139746470463296] #quality_metric: host=algo-1, epoch=0, batch=5 train loss <loss>=3.847811977068583\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:13:16 INFO 139746470463296] Epoch[0] Batch [5]#011Speed: 27.80 samples/sec#011loss=3.847812\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:13:28 INFO 139746470463296] Epoch[0] Batch[10] avg_epoch_loss=3.719460\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:13:28 INFO 139746470463296] #quality_metric: host=algo-1, epoch=0, batch=10 train loss <loss>=3.5654372215270995\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:13:28 INFO 139746470463296] Epoch[0] Batch [10]#011Speed: 27.28 samples/sec#011loss=3.565437\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:13:28 INFO 139746470463296] processed a total of 675 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"StartTime\": 1718136770.7371027, \"EndTime\": 1718136808.4785492, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"epochs\": {\"sum\": 400.0, \"count\": 1, \"min\": 400, \"max\": 400}, \"update.time\": {\"sum\": 37741.371393203735, \"count\": 1, \"min\": 37741.371393203735, \"max\": 37741.371393203735}}}\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:13:28 INFO 139746470463296] #throughput_metric: host=algo-1, train throughput=17.88482111705716 records/second\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:13:28 INFO 139746470463296] #progress_metric: host=algo-1, completed 0.25 % of epochs\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:13:28 INFO 139746470463296] #quality_metric: host=algo-1, epoch=0, train loss <loss>=3.719459815458818\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:13:28 INFO 139746470463296] best epoch loss so far\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:13:28 INFO 139746470463296] Saved checkpoint to \"/opt/ml/model/state_27a323db-2287-4292-8966-1e22cadb80ce-0000.params\"\u001b[0m\n",
      "\u001b[32m#metrics {\"StartTime\": 1718136808.4786472, \"EndTime\": 1718136808.7063458, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 227.35834121704102, \"count\": 1, \"min\": 227.35834121704102, \"max\": 227.35834121704102}}}\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:13:42 INFO 139746470463296] Epoch[1] Batch[0] avg_epoch_loss=3.562141\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:13:42 INFO 139746470463296] #quality_metric: host=algo-1, epoch=1, batch=0 train loss <loss>=3.562141180038452\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:13:53 INFO 139746470463296] Epoch[1] Batch[5] avg_epoch_loss=3.639069\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:13:53 INFO 139746470463296] #quality_metric: host=algo-1, epoch=1, batch=5 train loss <loss>=3.639069159825643\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:13:53 INFO 139746470463296] Epoch[1] Batch [5]#011Speed: 29.68 samples/sec#011loss=3.639069\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:14:05 INFO 139746470463296] Epoch[1] Batch[10] avg_epoch_loss=3.619114\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:14:05 INFO 139746470463296] #quality_metric: host=algo-1, epoch=1, batch=10 train loss <loss>=3.5951674938201905\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:14:05 INFO 139746470463296] Epoch[1] Batch [10]#011Speed: 27.21 samples/sec#011loss=3.595167\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:14:05 INFO 139746470463296] processed a total of 646 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"StartTime\": 1718136808.7064216, \"EndTime\": 1718136845.1007318, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 36394.24276351929, \"count\": 1, \"min\": 36394.24276351929, \"max\": 36394.24276351929}}}\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:14:05 INFO 139746470463296] #throughput_metric: host=algo-1, train throughput=17.750000432364352 records/second\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:14:05 INFO 139746470463296] #progress_metric: host=algo-1, completed 0.5 % of epochs\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:14:05 INFO 139746470463296] #quality_metric: host=algo-1, epoch=1, train loss <loss>=3.6191138570958916\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:14:05 INFO 139746470463296] best epoch loss so far\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:14:05 INFO 139746470463296] Saved checkpoint to \"/opt/ml/model/state_54918931-47d3-4779-a9bb-308e2946612a-0000.params\"\u001b[0m\n",
      "\u001b[32m#metrics {\"StartTime\": 1718136845.100819, \"EndTime\": 1718136845.305673, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 204.45799827575684, \"count\": 1, \"min\": 204.45799827575684, \"max\": 204.45799827575684}}}\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:14:19 INFO 139746470463296] Epoch[2] Batch[0] avg_epoch_loss=3.380656\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:14:19 INFO 139746470463296] #quality_metric: host=algo-1, epoch=2, batch=0 train loss <loss>=3.3806560039520264\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:14:29 INFO 139746470463296] Epoch[2] Batch[5] avg_epoch_loss=3.561531\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:14:29 INFO 139746470463296] #quality_metric: host=algo-1, epoch=2, batch=5 train loss <loss>=3.56153138478597\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:14:29 INFO 139746470463296] Epoch[2] Batch [5]#011Speed: 30.07 samples/sec#011loss=3.561531\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:14:38 INFO 139746470463296] processed a total of 622 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"StartTime\": 1718136845.3057313, \"EndTime\": 1718136878.3052468, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 32999.44734573364, \"count\": 1, \"min\": 32999.44734573364, \"max\": 32999.44734573364}}}\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:14:38 INFO 139746470463296] #throughput_metric: host=algo-1, train throughput=18.848736778628034 records/second\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:14:38 INFO 139746470463296] #progress_metric: host=algo-1, completed 0.75 % of epochs\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:14:38 INFO 139746470463296] #quality_metric: host=algo-1, epoch=2, train loss <loss>=3.5905744314193724\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:14:38 INFO 139746470463296] best epoch loss so far\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:14:38 INFO 139746470463296] Saved checkpoint to \"/opt/ml/model/state_4b11f667-52cb-408f-aaca-690d4a67a386-0000.params\"\u001b[0m\n",
      "\u001b[32m#metrics {\"StartTime\": 1718136878.3053217, \"EndTime\": 1718136878.4964638, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 190.61040878295898, \"count\": 1, \"min\": 190.61040878295898, \"max\": 190.61040878295898}}}\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:14:51 INFO 139746470463296] Epoch[3] Batch[0] avg_epoch_loss=3.444253\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:14:51 INFO 139746470463296] #quality_metric: host=algo-1, epoch=3, batch=0 train loss <loss>=3.4442532062530518\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:15:02 INFO 139746470463296] Epoch[3] Batch[5] avg_epoch_loss=3.404415\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:15:02 INFO 139746470463296] #quality_metric: host=algo-1, epoch=3, batch=5 train loss <loss>=3.404415170351664\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:15:02 INFO 139746470463296] Epoch[3] Batch [5]#011Speed: 30.49 samples/sec#011loss=3.404415\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:15:11 INFO 139746470463296] processed a total of 624 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"StartTime\": 1718136878.496534, \"EndTime\": 1718136911.0194473, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 32522.84812927246, \"count\": 1, \"min\": 32522.84812927246, \"max\": 32522.84812927246}}}\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:15:11 INFO 139746470463296] #throughput_metric: host=algo-1, train throughput=19.186447514227336 records/second\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:15:11 INFO 139746470463296] #progress_metric: host=algo-1, completed 1.0 % of epochs\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:15:11 INFO 139746470463296] #quality_metric: host=algo-1, epoch=3, train loss <loss>=3.4317148685455323\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:15:11 INFO 139746470463296] best epoch loss so far\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:15:11 INFO 139746470463296] Saved checkpoint to \"/opt/ml/model/state_f90cbe78-d9ea-40f4-b7b9-64d162b92b79-0000.params\"\u001b[0m\n",
      "\u001b[32m#metrics {\"StartTime\": 1718136911.0195234, \"EndTime\": 1718136911.2052944, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 185.31417846679688, \"count\": 1, \"min\": 185.31417846679688, \"max\": 185.31417846679688}}}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[06/11/2024 20:15:24 INFO 139746470463296] Epoch[4] Batch[0] avg_epoch_loss=3.655554\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:15:24 INFO 139746470463296] #quality_metric: host=algo-1, epoch=4, batch=0 train loss <loss>=3.6555542945861816\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:15:35 INFO 139746470463296] Epoch[4] Batch[5] avg_epoch_loss=3.529871\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:15:35 INFO 139746470463296] #quality_metric: host=algo-1, epoch=4, batch=5 train loss <loss>=3.5298712650934854\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:15:35 INFO 139746470463296] Epoch[4] Batch [5]#011Speed: 30.70 samples/sec#011loss=3.529871\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:15:43 INFO 139746470463296] processed a total of 601 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"StartTime\": 1718136911.2053545, \"EndTime\": 1718136943.3886774, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 32183.262825012207, \"count\": 1, \"min\": 32183.262825012207, \"max\": 32183.262825012207}}}\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:15:43 INFO 139746470463296] #throughput_metric: host=algo-1, train throughput=18.67422921662166 records/second\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:15:43 INFO 139746470463296] #progress_metric: host=algo-1, completed 1.25 % of epochs\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:15:43 INFO 139746470463296] #quality_metric: host=algo-1, epoch=4, train loss <loss>=3.3919673919677735\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:15:43 INFO 139746470463296] best epoch loss so far\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:15:43 INFO 139746470463296] Saved checkpoint to \"/opt/ml/model/state_540736ce-b745-48c4-a80c-4e2ea2cbecdc-0000.params\"\u001b[0m\n",
      "\u001b[32m#metrics {\"StartTime\": 1718136943.3887715, \"EndTime\": 1718136943.5739093, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 184.6778392791748, \"count\": 1, \"min\": 184.6778392791748, \"max\": 184.6778392791748}}}\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:15:56 INFO 139746470463296] Epoch[5] Batch[0] avg_epoch_loss=3.427539\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:15:56 INFO 139746470463296] #quality_metric: host=algo-1, epoch=5, batch=0 train loss <loss>=3.427539348602295\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:16:07 INFO 139746470463296] Epoch[5] Batch[5] avg_epoch_loss=3.480616\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:16:07 INFO 139746470463296] #quality_metric: host=algo-1, epoch=5, batch=5 train loss <loss>=3.480616013209025\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:16:07 INFO 139746470463296] Epoch[5] Batch [5]#011Speed: 30.64 samples/sec#011loss=3.480616\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:16:15 INFO 139746470463296] processed a total of 628 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"StartTime\": 1718136943.5739768, \"EndTime\": 1718136975.1015663, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 31527.517795562744, \"count\": 1, \"min\": 31527.517795562744, \"max\": 31527.517795562744}}}\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:16:15 INFO 139746470463296] #throughput_metric: host=algo-1, train throughput=19.919036490858325 records/second\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:16:15 INFO 139746470463296] #progress_metric: host=algo-1, completed 1.5 % of epochs\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:16:15 INFO 139746470463296] #quality_metric: host=algo-1, epoch=5, train loss <loss>=3.3794947624206544\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:16:15 INFO 139746470463296] best epoch loss so far\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:16:15 INFO 139746470463296] Saved checkpoint to \"/opt/ml/model/state_c3b28e46-2f28-4288-8a83-7c1da9b206f3-0000.params\"\u001b[0m\n",
      "\u001b[32m#metrics {\"StartTime\": 1718136975.1016366, \"EndTime\": 1718136975.281897, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 179.92520332336426, \"count\": 1, \"min\": 179.92520332336426, \"max\": 179.92520332336426}}}\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:16:28 INFO 139746470463296] Epoch[6] Batch[0] avg_epoch_loss=3.481422\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:16:28 INFO 139746470463296] #quality_metric: host=algo-1, epoch=6, batch=0 train loss <loss>=3.481421947479248\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:16:37 INFO 139746470463296] Epoch[6] Batch[5] avg_epoch_loss=3.343404\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:16:37 INFO 139746470463296] #quality_metric: host=algo-1, epoch=6, batch=5 train loss <loss>=3.343404173851013\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:16:37 INFO 139746470463296] Epoch[6] Batch [5]#011Speed: 33.15 samples/sec#011loss=3.343404\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:16:47 INFO 139746470463296] Epoch[6] Batch[10] avg_epoch_loss=3.337001\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:16:47 INFO 139746470463296] #quality_metric: host=algo-1, epoch=6, batch=10 train loss <loss>=3.329316997528076\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:16:47 INFO 139746470463296] Epoch[6] Batch [10]#011Speed: 31.75 samples/sec#011loss=3.329317\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:16:47 INFO 139746470463296] processed a total of 678 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"StartTime\": 1718136975.2819526, \"EndTime\": 1718137007.9469118, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 32664.894104003906, \"count\": 1, \"min\": 32664.894104003906, \"max\": 32664.894104003906}}}\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:16:47 INFO 139746470463296] #throughput_metric: host=algo-1, train throughput=20.75616377981253 records/second\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:16:47 INFO 139746470463296] #progress_metric: host=algo-1, completed 1.75 % of epochs\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:16:47 INFO 139746470463296] #quality_metric: host=algo-1, epoch=6, train loss <loss>=3.337000911886042\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:16:47 INFO 139746470463296] best epoch loss so far\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:16:48 INFO 139746470463296] Saved checkpoint to \"/opt/ml/model/state_de993457-3b5c-436e-b0af-673cd063393d-0000.params\"\u001b[0m\n",
      "\u001b[32m#metrics {\"StartTime\": 1718137007.946979, \"EndTime\": 1718137008.133854, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 186.45119667053223, \"count\": 1, \"min\": 186.45119667053223, \"max\": 186.45119667053223}}}\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:17:01 INFO 139746470463296] Epoch[7] Batch[0] avg_epoch_loss=3.263254\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:17:01 INFO 139746470463296] #quality_metric: host=algo-1, epoch=7, batch=0 train loss <loss>=3.263253927230835\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:17:11 INFO 139746470463296] Epoch[7] Batch[5] avg_epoch_loss=3.288797\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:17:11 INFO 139746470463296] #quality_metric: host=algo-1, epoch=7, batch=5 train loss <loss>=3.288796822230021\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:17:11 INFO 139746470463296] Epoch[7] Batch [5]#011Speed: 29.40 samples/sec#011loss=3.288797\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:17:20 INFO 139746470463296] processed a total of 596 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"StartTime\": 1718137008.1339118, \"EndTime\": 1718137040.2429154, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 32108.935117721558, \"count\": 1, \"min\": 32108.935117721558, \"max\": 32108.935117721558}}}\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:17:20 INFO 139746470463296] #throughput_metric: host=algo-1, train throughput=18.561749994418047 records/second\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:17:20 INFO 139746470463296] #progress_metric: host=algo-1, completed 2.0 % of epochs\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:17:20 INFO 139746470463296] #quality_metric: host=algo-1, epoch=7, train loss <loss>=3.255060577392578\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:17:20 INFO 139746470463296] best epoch loss so far\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:17:20 INFO 139746470463296] Saved checkpoint to \"/opt/ml/model/state_a53320b0-bbcd-4f67-bcb6-3b6244917b0b-0000.params\"\u001b[0m\n",
      "\u001b[32m#metrics {\"StartTime\": 1718137040.242987, \"EndTime\": 1718137040.429038, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 185.65654754638672, \"count\": 1, \"min\": 185.65654754638672, \"max\": 185.65654754638672}}}\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:17:33 INFO 139746470463296] Epoch[8] Batch[0] avg_epoch_loss=3.147975\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:17:33 INFO 139746470463296] #quality_metric: host=algo-1, epoch=8, batch=0 train loss <loss>=3.147975444793701\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:17:43 INFO 139746470463296] Epoch[8] Batch[5] avg_epoch_loss=3.176204\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:17:43 INFO 139746470463296] #quality_metric: host=algo-1, epoch=8, batch=5 train loss <loss>=3.176204244295756\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:17:43 INFO 139746470463296] Epoch[8] Batch [5]#011Speed: 31.85 samples/sec#011loss=3.176204\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[06/11/2024 20:17:51 INFO 139746470463296] processed a total of 619 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"StartTime\": 1718137040.4291031, \"EndTime\": 1718137071.9346907, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 31505.51986694336, \"count\": 1, \"min\": 31505.51986694336, \"max\": 31505.51986694336}}}\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:17:51 INFO 139746470463296] #throughput_metric: host=algo-1, train throughput=19.647283119530126 records/second\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:17:51 INFO 139746470463296] #progress_metric: host=algo-1, completed 2.25 % of epochs\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:17:51 INFO 139746470463296] #quality_metric: host=algo-1, epoch=8, train loss <loss>=3.1502495288848875\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:17:51 INFO 139746470463296] best epoch loss so far\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:17:52 INFO 139746470463296] Saved checkpoint to \"/opt/ml/model/state_83716e4e-4e36-4ba8-b670-e2293dd6093a-0000.params\"\u001b[0m\n",
      "\u001b[32m#metrics {\"StartTime\": 1718137071.9347649, \"EndTime\": 1718137072.124351, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 188.9171600341797, \"count\": 1, \"min\": 188.9171600341797, \"max\": 188.9171600341797}}}\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:18:05 INFO 139746470463296] Epoch[9] Batch[0] avg_epoch_loss=3.092406\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:18:05 INFO 139746470463296] #quality_metric: host=algo-1, epoch=9, batch=0 train loss <loss>=3.0924057960510254\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:18:16 INFO 139746470463296] Epoch[9] Batch[5] avg_epoch_loss=3.151467\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:18:16 INFO 139746470463296] #quality_metric: host=algo-1, epoch=9, batch=5 train loss <loss>=3.1514667669932046\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:18:16 INFO 139746470463296] Epoch[9] Batch [5]#011Speed: 29.25 samples/sec#011loss=3.151467\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:18:25 INFO 139746470463296] processed a total of 637 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"StartTime\": 1718137072.1244106, \"EndTime\": 1718137105.2668462, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 33142.37928390503, \"count\": 1, \"min\": 33142.37928390503, \"max\": 33142.37928390503}}}\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:18:25 INFO 139746470463296] #throughput_metric: host=algo-1, train throughput=19.220043038735582 records/second\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:18:25 INFO 139746470463296] #progress_metric: host=algo-1, completed 2.5 % of epochs\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:18:25 INFO 139746470463296] #quality_metric: host=algo-1, epoch=9, train loss <loss>=3.124663257598877\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:18:25 INFO 139746470463296] best epoch loss so far\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:18:25 INFO 139746470463296] Saved checkpoint to \"/opt/ml/model/state_fad6063b-fa83-46eb-ab65-6d5a9dac411e-0000.params\"\u001b[0m\n",
      "\u001b[32m#metrics {\"StartTime\": 1718137105.2669213, \"EndTime\": 1718137105.4485598, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 181.18000030517578, \"count\": 1, \"min\": 181.18000030517578, \"max\": 181.18000030517578}}}\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:18:39 INFO 139746470463296] Epoch[10] Batch[0] avg_epoch_loss=3.104795\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:18:39 INFO 139746470463296] #quality_metric: host=algo-1, epoch=10, batch=0 train loss <loss>=3.104794979095459\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:18:49 INFO 139746470463296] Epoch[10] Batch[5] avg_epoch_loss=3.125125\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:18:49 INFO 139746470463296] #quality_metric: host=algo-1, epoch=10, batch=5 train loss <loss>=3.1251253684361777\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:18:49 INFO 139746470463296] Epoch[10] Batch [5]#011Speed: 30.81 samples/sec#011loss=3.125125\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:19:00 INFO 139746470463296] Epoch[10] Batch[10] avg_epoch_loss=3.053354\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:19:00 INFO 139746470463296] #quality_metric: host=algo-1, epoch=10, batch=10 train loss <loss>=2.967227411270142\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:19:00 INFO 139746470463296] Epoch[10] Batch [10]#011Speed: 29.90 samples/sec#011loss=2.967227\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:19:00 INFO 139746470463296] processed a total of 660 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"StartTime\": 1718137105.4486291, \"EndTime\": 1718137140.1054168, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 34656.73089027405, \"count\": 1, \"min\": 34656.73089027405, \"max\": 34656.73089027405}}}\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:19:00 INFO 139746470463296] #throughput_metric: host=algo-1, train throughput=19.04386493341559 records/second\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:19:00 INFO 139746470463296] #progress_metric: host=algo-1, completed 2.75 % of epochs\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:19:00 INFO 139746470463296] #quality_metric: host=algo-1, epoch=10, train loss <loss>=3.0533535697243432\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:19:00 INFO 139746470463296] best epoch loss so far\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:19:00 INFO 139746470463296] Saved checkpoint to \"/opt/ml/model/state_ca17b97d-a378-4870-9d2f-b70415712acd-0000.params\"\u001b[0m\n",
      "\u001b[32m#metrics {\"StartTime\": 1718137140.1054857, \"EndTime\": 1718137140.2887077, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 182.71422386169434, \"count\": 1, \"min\": 182.71422386169434, \"max\": 182.71422386169434}}}\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:19:13 INFO 139746470463296] Epoch[11] Batch[0] avg_epoch_loss=3.108596\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:19:13 INFO 139746470463296] #quality_metric: host=algo-1, epoch=11, batch=0 train loss <loss>=3.108595848083496\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:19:24 INFO 139746470463296] Epoch[11] Batch[5] avg_epoch_loss=3.025174\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:19:24 INFO 139746470463296] #quality_metric: host=algo-1, epoch=11, batch=5 train loss <loss>=3.0251744985580444\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:19:24 INFO 139746470463296] Epoch[11] Batch [5]#011Speed: 30.83 samples/sec#011loss=3.025174\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:19:33 INFO 139746470463296] processed a total of 622 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"StartTime\": 1718137140.2887707, \"EndTime\": 1718137173.0608597, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 32772.0263004303, \"count\": 1, \"min\": 32772.0263004303, \"max\": 32772.0263004303}}}\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:19:33 INFO 139746470463296] #throughput_metric: host=algo-1, train throughput=18.979543659924982 records/second\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:19:33 INFO 139746470463296] #progress_metric: host=algo-1, completed 3.0 % of epochs\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:19:33 INFO 139746470463296] #quality_metric: host=algo-1, epoch=11, train loss <loss>=3.0445878744125365\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:19:33 INFO 139746470463296] best epoch loss so far\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:19:33 INFO 139746470463296] Saved checkpoint to \"/opt/ml/model/state_b3ebceed-8e7c-4a4a-97b1-2b102db8769c-0000.params\"\u001b[0m\n",
      "\u001b[32m#metrics {\"StartTime\": 1718137173.0609298, \"EndTime\": 1718137173.250561, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 189.29553031921387, \"count\": 1, \"min\": 189.29553031921387, \"max\": 189.29553031921387}}}\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:19:46 INFO 139746470463296] Epoch[12] Batch[0] avg_epoch_loss=3.038217\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:19:46 INFO 139746470463296] #quality_metric: host=algo-1, epoch=12, batch=0 train loss <loss>=3.038217306137085\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:19:56 INFO 139746470463296] Epoch[12] Batch[5] avg_epoch_loss=2.970468\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:19:56 INFO 139746470463296] #quality_metric: host=algo-1, epoch=12, batch=5 train loss <loss>=2.9704678853352866\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:19:56 INFO 139746470463296] Epoch[12] Batch [5]#011Speed: 30.86 samples/sec#011loss=2.970468\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:20:08 INFO 139746470463296] Epoch[12] Batch[10] avg_epoch_loss=2.928087\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:20:08 INFO 139746470463296] #quality_metric: host=algo-1, epoch=12, batch=10 train loss <loss>=2.8772292137145996\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:20:08 INFO 139746470463296] Epoch[12] Batch [10]#011Speed: 28.08 samples/sec#011loss=2.877229\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:20:08 INFO 139746470463296] processed a total of 668 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"StartTime\": 1718137173.2506557, \"EndTime\": 1718137208.3252556, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 35074.535846710205, \"count\": 1, \"min\": 35074.535846710205, \"max\": 35074.535846710205}}}\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:20:08 INFO 139746470463296] #throughput_metric: host=algo-1, train throughput=19.045101562101447 records/second\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:20:08 INFO 139746470463296] #progress_metric: host=algo-1, completed 3.25 % of epochs\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:20:08 INFO 139746470463296] #quality_metric: host=algo-1, epoch=12, train loss <loss>=2.928086670962247\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:20:08 INFO 139746470463296] best epoch loss so far\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:20:08 INFO 139746470463296] Saved checkpoint to \"/opt/ml/model/state_8912887c-7ad3-42b1-a7fb-2d89bd045f74-0000.params\"\u001b[0m\n",
      "\u001b[32m#metrics {\"StartTime\": 1718137208.3253236, \"EndTime\": 1718137208.5076423, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 181.83493614196777, \"count\": 1, \"min\": 181.83493614196777, \"max\": 181.83493614196777}}}\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:20:21 INFO 139746470463296] Epoch[13] Batch[0] avg_epoch_loss=2.944911\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:20:21 INFO 139746470463296] #quality_metric: host=algo-1, epoch=13, batch=0 train loss <loss>=2.944911003112793\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:20:32 INFO 139746470463296] Epoch[13] Batch[5] avg_epoch_loss=2.934209\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:20:32 INFO 139746470463296] #quality_metric: host=algo-1, epoch=13, batch=5 train loss <loss>=2.9342090686162314\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:20:32 INFO 139746470463296] Epoch[13] Batch [5]#011Speed: 30.94 samples/sec#011loss=2.934209\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[06/11/2024 20:20:40 INFO 139746470463296] processed a total of 632 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"StartTime\": 1718137208.5077076, \"EndTime\": 1718137240.6076372, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 32099.8694896698, \"count\": 1, \"min\": 32099.8694896698, \"max\": 32099.8694896698}}}\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:20:40 INFO 139746470463296] #throughput_metric: host=algo-1, train throughput=19.68848630107231 records/second\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:20:40 INFO 139746470463296] #progress_metric: host=algo-1, completed 3.5 % of epochs\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:20:40 INFO 139746470463296] #quality_metric: host=algo-1, epoch=13, train loss <loss>=2.935241937637329\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:20:40 INFO 139746470463296] loss did not improve\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:20:54 INFO 139746470463296] Epoch[14] Batch[0] avg_epoch_loss=2.918228\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:20:54 INFO 139746470463296] #quality_metric: host=algo-1, epoch=14, batch=0 train loss <loss>=2.9182276725769043\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:21:05 INFO 139746470463296] Epoch[14] Batch[5] avg_epoch_loss=2.906021\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:21:05 INFO 139746470463296] #quality_metric: host=algo-1, epoch=14, batch=5 train loss <loss>=2.906020681063334\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:21:05 INFO 139746470463296] Epoch[14] Batch [5]#011Speed: 29.05 samples/sec#011loss=2.906021\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:21:13 INFO 139746470463296] processed a total of 612 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"StartTime\": 1718137240.6077144, \"EndTime\": 1718137273.4309084, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 32822.67785072327, \"count\": 1, \"min\": 32822.67785072327, \"max\": 32822.67785072327}}}\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:21:13 INFO 139746470463296] #throughput_metric: host=algo-1, train throughput=18.645576087618632 records/second\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:21:13 INFO 139746470463296] #progress_metric: host=algo-1, completed 3.75 % of epochs\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:21:13 INFO 139746470463296] #quality_metric: host=algo-1, epoch=14, train loss <loss>=2.9534327507019045\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:21:13 INFO 139746470463296] loss did not improve\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:21:26 INFO 139746470463296] Epoch[15] Batch[0] avg_epoch_loss=2.795637\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:21:26 INFO 139746470463296] #quality_metric: host=algo-1, epoch=15, batch=0 train loss <loss>=2.7956368923187256\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:21:37 INFO 139746470463296] Epoch[15] Batch[5] avg_epoch_loss=2.905050\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:21:37 INFO 139746470463296] #quality_metric: host=algo-1, epoch=15, batch=5 train loss <loss>=2.905049761136373\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:21:37 INFO 139746470463296] Epoch[15] Batch [5]#011Speed: 31.14 samples/sec#011loss=2.905050\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:21:43 INFO 139746470463296] processed a total of 564 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"StartTime\": 1718137273.430992, \"EndTime\": 1718137303.3582013, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 29926.777839660645, \"count\": 1, \"min\": 29926.777839660645, \"max\": 29926.777839660645}}}\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:21:43 INFO 139746470463296] #throughput_metric: host=algo-1, train throughput=18.845926539645117 records/second\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:21:43 INFO 139746470463296] #progress_metric: host=algo-1, completed 4.0 % of epochs\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:21:43 INFO 139746470463296] #quality_metric: host=algo-1, epoch=15, train loss <loss>=2.9329910543229847\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:21:43 INFO 139746470463296] loss did not improve\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:21:56 INFO 139746470463296] Epoch[16] Batch[0] avg_epoch_loss=2.983671\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:21:56 INFO 139746470463296] #quality_metric: host=algo-1, epoch=16, batch=0 train loss <loss>=2.983670711517334\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:22:07 INFO 139746470463296] Epoch[16] Batch[5] avg_epoch_loss=2.918747\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:22:07 INFO 139746470463296] #quality_metric: host=algo-1, epoch=16, batch=5 train loss <loss>=2.918746550877889\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:22:07 INFO 139746470463296] Epoch[16] Batch [5]#011Speed: 29.30 samples/sec#011loss=2.918747\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:22:15 INFO 139746470463296] processed a total of 606 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"StartTime\": 1718137303.358278, \"EndTime\": 1718137335.8997953, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 32541.079998016357, \"count\": 1, \"min\": 32541.079998016357, \"max\": 32541.079998016357}}}\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:22:15 INFO 139746470463296] #throughput_metric: host=algo-1, train throughput=18.622532314924264 records/second\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:22:15 INFO 139746470463296] #progress_metric: host=algo-1, completed 4.25 % of epochs\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:22:15 INFO 139746470463296] #quality_metric: host=algo-1, epoch=16, train loss <loss>=2.9680070161819456\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:22:15 INFO 139746470463296] loss did not improve\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:22:29 INFO 139746470463296] Epoch[17] Batch[0] avg_epoch_loss=2.864978\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:22:29 INFO 139746470463296] #quality_metric: host=algo-1, epoch=17, batch=0 train loss <loss>=2.8649775981903076\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:22:39 INFO 139746470463296] Epoch[17] Batch[5] avg_epoch_loss=2.832930\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:22:39 INFO 139746470463296] #quality_metric: host=algo-1, epoch=17, batch=5 train loss <loss>=2.8329304456710815\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:22:39 INFO 139746470463296] Epoch[17] Batch [5]#011Speed: 30.84 samples/sec#011loss=2.832930\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:22:50 INFO 139746470463296] Epoch[17] Batch[10] avg_epoch_loss=2.880786\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:22:50 INFO 139746470463296] #quality_metric: host=algo-1, epoch=17, batch=10 train loss <loss>=2.9382121086120607\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:22:50 INFO 139746470463296] Epoch[17] Batch [10]#011Speed: 30.03 samples/sec#011loss=2.938212\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:22:50 INFO 139746470463296] processed a total of 650 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"StartTime\": 1718137335.8999045, \"EndTime\": 1718137370.039619, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 34139.21236991882, \"count\": 1, \"min\": 34139.21236991882, \"max\": 34139.21236991882}}}\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:22:50 INFO 139746470463296] #throughput_metric: host=algo-1, train throughput=19.039551117901564 records/second\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:22:50 INFO 139746470463296] #progress_metric: host=algo-1, completed 4.5 % of epochs\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:22:50 INFO 139746470463296] #quality_metric: host=algo-1, epoch=17, train loss <loss>=2.88078574700789\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:22:50 INFO 139746470463296] best epoch loss so far\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:22:50 INFO 139746470463296] Saved checkpoint to \"/opt/ml/model/state_1e97ddba-e861-4c68-afce-e229a6b1e410-0000.params\"\u001b[0m\n",
      "\u001b[32m#metrics {\"StartTime\": 1718137370.0398307, \"EndTime\": 1718137370.2298696, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 189.61405754089355, \"count\": 1, \"min\": 189.61405754089355, \"max\": 189.61405754089355}}}\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:23:04 INFO 139746470463296] Epoch[18] Batch[0] avg_epoch_loss=2.883731\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:23:04 INFO 139746470463296] #quality_metric: host=algo-1, epoch=18, batch=0 train loss <loss>=2.8837311267852783\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:23:14 INFO 139746470463296] Epoch[18] Batch[5] avg_epoch_loss=2.869841\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:23:14 INFO 139746470463296] #quality_metric: host=algo-1, epoch=18, batch=5 train loss <loss>=2.8698412577311196\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:23:14 INFO 139746470463296] Epoch[18] Batch [5]#011Speed: 30.67 samples/sec#011loss=2.869841\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:23:22 INFO 139746470463296] processed a total of 640 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"StartTime\": 1718137370.2299407, \"EndTime\": 1718137402.9502344, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 32720.23034095764, \"count\": 1, \"min\": 32720.23034095764, \"max\": 32720.23034095764}}}\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:23:22 INFO 139746470463296] #throughput_metric: host=algo-1, train throughput=19.559699798848065 records/second\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:23:22 INFO 139746470463296] #progress_metric: host=algo-1, completed 4.75 % of epochs\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:23:22 INFO 139746470463296] #quality_metric: host=algo-1, epoch=18, train loss <loss>=2.8412901401519775\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:23:22 INFO 139746470463296] best epoch loss so far\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:23:23 INFO 139746470463296] Saved checkpoint to \"/opt/ml/model/state_21ad2a78-5e07-4c93-ac37-567d4ac45bee-0000.params\"\u001b[0m\n",
      "\u001b[32m#metrics {\"StartTime\": 1718137402.95031, \"EndTime\": 1718137403.1353686, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 184.48948860168457, \"count\": 1, \"min\": 184.48948860168457, \"max\": 184.48948860168457}}}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[06/11/2024 20:23:36 INFO 139746470463296] Epoch[19] Batch[0] avg_epoch_loss=2.824089\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:23:36 INFO 139746470463296] #quality_metric: host=algo-1, epoch=19, batch=0 train loss <loss>=2.8240888118743896\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:23:47 INFO 139746470463296] Epoch[19] Batch[5] avg_epoch_loss=2.878533\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:23:47 INFO 139746470463296] #quality_metric: host=algo-1, epoch=19, batch=5 train loss <loss>=2.8785329262415567\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:23:47 INFO 139746470463296] Epoch[19] Batch [5]#011Speed: 30.70 samples/sec#011loss=2.878533\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:23:55 INFO 139746470463296] processed a total of 602 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"StartTime\": 1718137403.1354418, \"EndTime\": 1718137435.5566168, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 32421.11325263977, \"count\": 1, \"min\": 32421.11325263977, \"max\": 32421.11325263977}}}\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:23:55 INFO 139746470463296] #throughput_metric: host=algo-1, train throughput=18.568089340347118 records/second\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:23:55 INFO 139746470463296] #progress_metric: host=algo-1, completed 5.0 % of epochs\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:23:55 INFO 139746470463296] #quality_metric: host=algo-1, epoch=19, train loss <loss>=2.824135994911194\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:23:55 INFO 139746470463296] best epoch loss so far\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:23:55 INFO 139746470463296] Saved checkpoint to \"/opt/ml/model/state_3559175f-8212-417d-ad11-514524294417-0000.params\"\u001b[0m\n",
      "\u001b[32m#metrics {\"StartTime\": 1718137435.5566862, \"EndTime\": 1718137435.7491856, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 191.94579124450684, \"count\": 1, \"min\": 191.94579124450684, \"max\": 191.94579124450684}}}\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:24:08 INFO 139746470463296] Epoch[20] Batch[0] avg_epoch_loss=2.805882\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:24:08 INFO 139746470463296] #quality_metric: host=algo-1, epoch=20, batch=0 train loss <loss>=2.8058815002441406\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:24:19 INFO 139746470463296] Epoch[20] Batch[5] avg_epoch_loss=2.840730\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:24:19 INFO 139746470463296] #quality_metric: host=algo-1, epoch=20, batch=5 train loss <loss>=2.84072999159495\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:24:19 INFO 139746470463296] Epoch[20] Batch [5]#011Speed: 30.73 samples/sec#011loss=2.840730\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:24:30 INFO 139746470463296] Epoch[20] Batch[10] avg_epoch_loss=2.817274\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:24:30 INFO 139746470463296] #quality_metric: host=algo-1, epoch=20, batch=10 train loss <loss>=2.789125919342041\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:24:30 INFO 139746470463296] Epoch[20] Batch [10]#011Speed: 28.53 samples/sec#011loss=2.789126\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:24:30 INFO 139746470463296] processed a total of 676 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"StartTime\": 1718137435.7492464, \"EndTime\": 1718137470.4508612, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 34701.550006866455, \"count\": 1, \"min\": 34701.550006866455, \"max\": 34701.550006866455}}}\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:24:30 INFO 139746470463296] #throughput_metric: host=algo-1, train throughput=19.480346317175766 records/second\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:24:30 INFO 139746470463296] #progress_metric: host=algo-1, completed 5.25 % of epochs\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:24:30 INFO 139746470463296] #quality_metric: host=algo-1, epoch=20, train loss <loss>=2.817273595116355\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:24:30 INFO 139746470463296] best epoch loss so far\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:24:30 INFO 139746470463296] Saved checkpoint to \"/opt/ml/model/state_df67d2c7-16ff-4591-834e-a4c5cef62745-0000.params\"\u001b[0m\n",
      "\u001b[32m#metrics {\"StartTime\": 1718137470.4509227, \"EndTime\": 1718137470.6465034, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 195.2042579650879, \"count\": 1, \"min\": 195.2042579650879, \"max\": 195.2042579650879}}}\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:24:44 INFO 139746470463296] Epoch[21] Batch[0] avg_epoch_loss=2.869568\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:24:44 INFO 139746470463296] #quality_metric: host=algo-1, epoch=21, batch=0 train loss <loss>=2.869568347930908\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:24:54 INFO 139746470463296] Epoch[21] Batch[5] avg_epoch_loss=2.852194\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:24:54 INFO 139746470463296] #quality_metric: host=algo-1, epoch=21, batch=5 train loss <loss>=2.852193911870321\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:24:54 INFO 139746470463296] Epoch[21] Batch [5]#011Speed: 31.05 samples/sec#011loss=2.852194\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:25:03 INFO 139746470463296] processed a total of 634 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"StartTime\": 1718137470.6465683, \"EndTime\": 1718137503.4327586, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 32786.10444068909, \"count\": 1, \"min\": 32786.10444068909, \"max\": 32786.10444068909}}}\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:25:03 INFO 139746470463296] #throughput_metric: host=algo-1, train throughput=19.33740145251755 records/second\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:25:03 INFO 139746470463296] #progress_metric: host=algo-1, completed 5.5 % of epochs\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:25:03 INFO 139746470463296] #quality_metric: host=algo-1, epoch=21, train loss <loss>=2.849741005897522\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:25:03 INFO 139746470463296] loss did not improve\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:25:16 INFO 139746470463296] Epoch[22] Batch[0] avg_epoch_loss=2.801853\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:25:16 INFO 139746470463296] #quality_metric: host=algo-1, epoch=22, batch=0 train loss <loss>=2.8018529415130615\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:25:26 INFO 139746470463296] Epoch[22] Batch[5] avg_epoch_loss=2.762115\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:25:26 INFO 139746470463296] #quality_metric: host=algo-1, epoch=22, batch=5 train loss <loss>=2.762115240097046\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:25:26 INFO 139746470463296] Epoch[22] Batch [5]#011Speed: 30.97 samples/sec#011loss=2.762115\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:25:37 INFO 139746470463296] Epoch[22] Batch[10] avg_epoch_loss=2.786768\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:25:37 INFO 139746470463296] #quality_metric: host=algo-1, epoch=22, batch=10 train loss <loss>=2.816350984573364\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:25:37 INFO 139746470463296] Epoch[22] Batch [10]#011Speed: 28.83 samples/sec#011loss=2.816351\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:25:37 INFO 139746470463296] processed a total of 673 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"StartTime\": 1718137503.4328272, \"EndTime\": 1718137537.9465854, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 34513.43846321106, \"count\": 1, \"min\": 34513.43846321106, \"max\": 34513.43846321106}}}\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:25:37 INFO 139746470463296] #throughput_metric: host=algo-1, train throughput=19.499576638374123 records/second\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:25:37 INFO 139746470463296] #progress_metric: host=algo-1, completed 5.75 % of epochs\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:25:37 INFO 139746470463296] #quality_metric: host=algo-1, epoch=22, train loss <loss>=2.786767851222645\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:25:37 INFO 139746470463296] best epoch loss so far\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:25:38 INFO 139746470463296] Saved checkpoint to \"/opt/ml/model/state_40c5fda4-1b14-4174-bc25-22f2d4a28e29-0000.params\"\u001b[0m\n",
      "\u001b[32m#metrics {\"StartTime\": 1718137537.9466834, \"EndTime\": 1718137538.1382482, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 191.12443923950195, \"count\": 1, \"min\": 191.12443923950195, \"max\": 191.12443923950195}}}\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:25:51 INFO 139746470463296] Epoch[23] Batch[0] avg_epoch_loss=2.907212\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:25:51 INFO 139746470463296] #quality_metric: host=algo-1, epoch=23, batch=0 train loss <loss>=2.9072115421295166\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:26:02 INFO 139746470463296] Epoch[23] Batch[5] avg_epoch_loss=2.805207\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:26:02 INFO 139746470463296] #quality_metric: host=algo-1, epoch=23, batch=5 train loss <loss>=2.805207053820292\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:26:02 INFO 139746470463296] Epoch[23] Batch [5]#011Speed: 30.69 samples/sec#011loss=2.805207\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[06/11/2024 20:26:10 INFO 139746470463296] processed a total of 637 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"StartTime\": 1718137538.1383197, \"EndTime\": 1718137570.6082559, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 32469.874143600464, \"count\": 1, \"min\": 32469.874143600464, \"max\": 32469.874143600464}}}\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:26:10 INFO 139746470463296] #throughput_metric: host=algo-1, train throughput=19.6181211066826 records/second\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:26:10 INFO 139746470463296] #progress_metric: host=algo-1, completed 6.0 % of epochs\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:26:10 INFO 139746470463296] #quality_metric: host=algo-1, epoch=23, train loss <loss>=2.798051381111145\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:26:10 INFO 139746470463296] loss did not improve\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:26:23 INFO 139746470463296] Epoch[24] Batch[0] avg_epoch_loss=2.841030\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:26:23 INFO 139746470463296] #quality_metric: host=algo-1, epoch=24, batch=0 train loss <loss>=2.8410301208496094\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:26:33 INFO 139746470463296] Epoch[24] Batch[5] avg_epoch_loss=2.817733\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:26:33 INFO 139746470463296] #quality_metric: host=algo-1, epoch=24, batch=5 train loss <loss>=2.817732810974121\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:26:33 INFO 139746470463296] Epoch[24] Batch [5]#011Speed: 31.19 samples/sec#011loss=2.817733\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:26:44 INFO 139746470463296] Epoch[24] Batch[10] avg_epoch_loss=2.833472\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:26:44 INFO 139746470463296] #quality_metric: host=algo-1, epoch=24, batch=10 train loss <loss>=2.8523598670959474\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:26:44 INFO 139746470463296] Epoch[24] Batch [10]#011Speed: 29.65 samples/sec#011loss=2.852360\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:26:44 INFO 139746470463296] processed a total of 659 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"StartTime\": 1718137570.608328, \"EndTime\": 1718137604.6239235, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 34015.20299911499, \"count\": 1, \"min\": 34015.20299911499, \"max\": 34015.20299911499}}}\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:26:44 INFO 139746470463296] #throughput_metric: host=algo-1, train throughput=19.373639402265955 records/second\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:26:44 INFO 139746470463296] #progress_metric: host=algo-1, completed 6.25 % of epochs\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:26:44 INFO 139746470463296] #quality_metric: host=algo-1, epoch=24, train loss <loss>=2.8334723819385874\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:26:44 INFO 139746470463296] loss did not improve\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:26:57 INFO 139746470463296] Epoch[25] Batch[0] avg_epoch_loss=2.622491\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:26:57 INFO 139746470463296] #quality_metric: host=algo-1, epoch=25, batch=0 train loss <loss>=2.6224911212921143\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:27:08 INFO 139746470463296] Epoch[25] Batch[5] avg_epoch_loss=2.776970\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:27:08 INFO 139746470463296] #quality_metric: host=algo-1, epoch=25, batch=5 train loss <loss>=2.7769695520401\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:27:08 INFO 139746470463296] Epoch[25] Batch [5]#011Speed: 29.56 samples/sec#011loss=2.776970\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:27:16 INFO 139746470463296] processed a total of 636 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"StartTime\": 1718137604.6239853, \"EndTime\": 1718137636.9178333, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 32293.575286865234, \"count\": 1, \"min\": 32293.575286865234, \"max\": 32293.575286865234}}}\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:27:16 INFO 139746470463296] #throughput_metric: host=algo-1, train throughput=19.694253813778573 records/second\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:27:16 INFO 139746470463296] #progress_metric: host=algo-1, completed 6.5 % of epochs\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:27:16 INFO 139746470463296] #quality_metric: host=algo-1, epoch=25, train loss <loss>=2.764168643951416\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:27:16 INFO 139746470463296] best epoch loss so far\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:27:17 INFO 139746470463296] Saved checkpoint to \"/opt/ml/model/state_ef8a3623-5f87-4ea3-a784-92eccdf0c626-0000.params\"\u001b[0m\n",
      "\u001b[32m#metrics {\"StartTime\": 1718137636.917909, \"EndTime\": 1718137637.1031556, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 184.77344512939453, \"count\": 1, \"min\": 184.77344512939453, \"max\": 184.77344512939453}}}\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:27:29 INFO 139746470463296] Epoch[26] Batch[0] avg_epoch_loss=2.781487\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:27:29 INFO 139746470463296] #quality_metric: host=algo-1, epoch=26, batch=0 train loss <loss>=2.781487464904785\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:27:40 INFO 139746470463296] Epoch[26] Batch[5] avg_epoch_loss=2.779008\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:27:40 INFO 139746470463296] #quality_metric: host=algo-1, epoch=26, batch=5 train loss <loss>=2.7790079911549888\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:27:40 INFO 139746470463296] Epoch[26] Batch [5]#011Speed: 31.32 samples/sec#011loss=2.779008\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:27:50 INFO 139746470463296] Epoch[26] Batch[10] avg_epoch_loss=2.862798\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:27:50 INFO 139746470463296] #quality_metric: host=algo-1, epoch=26, batch=10 train loss <loss>=2.9633452892303467\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:27:50 INFO 139746470463296] Epoch[26] Batch [10]#011Speed: 30.26 samples/sec#011loss=2.963345\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:27:50 INFO 139746470463296] processed a total of 650 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"StartTime\": 1718137637.1032143, \"EndTime\": 1718137670.7240732, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 33620.803117752075, \"count\": 1, \"min\": 33620.803117752075, \"max\": 33620.803117752075}}}\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:27:50 INFO 139746470463296] #throughput_metric: host=algo-1, train throughput=19.333214755989342 records/second\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:27:50 INFO 139746470463296] #progress_metric: host=algo-1, completed 6.75 % of epochs\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:27:50 INFO 139746470463296] #quality_metric: host=algo-1, epoch=26, train loss <loss>=2.862797672098333\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:27:50 INFO 139746470463296] loss did not improve\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:28:04 INFO 139746470463296] Epoch[27] Batch[0] avg_epoch_loss=2.780459\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:28:04 INFO 139746470463296] #quality_metric: host=algo-1, epoch=27, batch=0 train loss <loss>=2.780458688735962\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:28:14 INFO 139746470463296] Epoch[27] Batch[5] avg_epoch_loss=2.758877\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:28:14 INFO 139746470463296] #quality_metric: host=algo-1, epoch=27, batch=5 train loss <loss>=2.7588773568471274\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:28:14 INFO 139746470463296] Epoch[27] Batch [5]#011Speed: 30.93 samples/sec#011loss=2.758877\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:28:25 INFO 139746470463296] Epoch[27] Batch[10] avg_epoch_loss=2.688106\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:28:25 INFO 139746470463296] #quality_metric: host=algo-1, epoch=27, batch=10 train loss <loss>=2.603179383277893\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:28:25 INFO 139746470463296] Epoch[27] Batch [10]#011Speed: 30.01 samples/sec#011loss=2.603179\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:28:25 INFO 139746470463296] processed a total of 649 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"StartTime\": 1718137670.724137, \"EndTime\": 1718137705.371812, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 34647.24111557007, \"count\": 1, \"min\": 34647.24111557007, \"max\": 34647.24111557007}}}\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:28:25 INFO 139746470463296] #throughput_metric: host=algo-1, train throughput=18.731595905885435 records/second\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:28:25 INFO 139746470463296] #progress_metric: host=algo-1, completed 7.0 % of epochs\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:28:25 INFO 139746470463296] #quality_metric: host=algo-1, epoch=27, train loss <loss>=2.6881055506792935\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:28:25 INFO 139746470463296] best epoch loss so far\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:28:25 INFO 139746470463296] Saved checkpoint to \"/opt/ml/model/state_d9fd9f10-ff9a-4841-bd9f-3ccec79619e3-0000.params\"\u001b[0m\n",
      "\u001b[32m#metrics {\"StartTime\": 1718137705.371879, \"EndTime\": 1718137705.5542428, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 181.78439140319824, \"count\": 1, \"min\": 181.78439140319824, \"max\": 181.78439140319824}}}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[06/11/2024 20:28:38 INFO 139746470463296] Epoch[28] Batch[0] avg_epoch_loss=2.629813\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:28:38 INFO 139746470463296] #quality_metric: host=algo-1, epoch=28, batch=0 train loss <loss>=2.6298134326934814\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:28:48 INFO 139746470463296] Epoch[28] Batch[5] avg_epoch_loss=2.749303\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:28:48 INFO 139746470463296] #quality_metric: host=algo-1, epoch=28, batch=5 train loss <loss>=2.749303142229716\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:28:48 INFO 139746470463296] Epoch[28] Batch [5]#011Speed: 31.27 samples/sec#011loss=2.749303\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:28:59 INFO 139746470463296] Epoch[28] Batch[10] avg_epoch_loss=2.730471\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:28:59 INFO 139746470463296] #quality_metric: host=algo-1, epoch=28, batch=10 train loss <loss>=2.7078726291656494\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:28:59 INFO 139746470463296] Epoch[28] Batch [10]#011Speed: 29.39 samples/sec#011loss=2.707873\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:28:59 INFO 139746470463296] processed a total of 676 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"StartTime\": 1718137705.5543096, \"EndTime\": 1718137739.3928387, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 33838.47379684448, \"count\": 1, \"min\": 33838.47379684448, \"max\": 33838.47379684448}}}\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:28:59 INFO 139746470463296] #throughput_metric: host=algo-1, train throughput=19.97720697763269 records/second\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:28:59 INFO 139746470463296] #progress_metric: host=algo-1, completed 7.25 % of epochs\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:28:59 INFO 139746470463296] #quality_metric: host=algo-1, epoch=28, train loss <loss>=2.7304710908369585\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:28:59 INFO 139746470463296] loss did not improve\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:29:12 INFO 139746470463296] Epoch[29] Batch[0] avg_epoch_loss=2.704414\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:29:12 INFO 139746470463296] #quality_metric: host=algo-1, epoch=29, batch=0 train loss <loss>=2.704413890838623\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:29:22 INFO 139746470463296] Epoch[29] Batch[5] avg_epoch_loss=2.753516\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:29:22 INFO 139746470463296] #quality_metric: host=algo-1, epoch=29, batch=5 train loss <loss>=2.753515601158142\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:29:22 INFO 139746470463296] Epoch[29] Batch [5]#011Speed: 31.26 samples/sec#011loss=2.753516\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:29:33 INFO 139746470463296] Epoch[29] Batch[10] avg_epoch_loss=2.845517\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:29:33 INFO 139746470463296] #quality_metric: host=algo-1, epoch=29, batch=10 train loss <loss>=2.9559178829193113\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:29:33 INFO 139746470463296] Epoch[29] Batch [10]#011Speed: 29.94 samples/sec#011loss=2.955918\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:29:33 INFO 139746470463296] processed a total of 657 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"StartTime\": 1718137739.3928988, \"EndTime\": 1718137773.4674644, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 34074.18608665466, \"count\": 1, \"min\": 34074.18608665466, \"max\": 34074.18608665466}}}\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:29:33 INFO 139746470463296] #throughput_metric: host=algo-1, train throughput=19.281398858153114 records/second\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:29:33 INFO 139746470463296] #progress_metric: host=algo-1, completed 7.5 % of epochs\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:29:33 INFO 139746470463296] #quality_metric: host=algo-1, epoch=29, train loss <loss>=2.84551663832231\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:29:33 INFO 139746470463296] loss did not improve\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:29:46 INFO 139746470463296] Epoch[30] Batch[0] avg_epoch_loss=2.730625\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:29:46 INFO 139746470463296] #quality_metric: host=algo-1, epoch=30, batch=0 train loss <loss>=2.7306249141693115\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:29:56 INFO 139746470463296] Epoch[30] Batch[5] avg_epoch_loss=2.753806\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:29:56 INFO 139746470463296] #quality_metric: host=algo-1, epoch=30, batch=5 train loss <loss>=2.753805677096049\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:29:56 INFO 139746470463296] Epoch[30] Batch [5]#011Speed: 31.53 samples/sec#011loss=2.753806\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:30:05 INFO 139746470463296] processed a total of 600 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"StartTime\": 1718137773.4675348, \"EndTime\": 1718137805.9252963, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 32457.41844177246, \"count\": 1, \"min\": 32457.41844177246, \"max\": 32457.41844177246}}}\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:30:05 INFO 139746470463296] #throughput_metric: host=algo-1, train throughput=18.48569783817412 records/second\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:30:05 INFO 139746470463296] #progress_metric: host=algo-1, completed 7.75 % of epochs\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:30:05 INFO 139746470463296] #quality_metric: host=algo-1, epoch=30, train loss <loss>=2.782850503921509\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:30:05 INFO 139746470463296] loss did not improve\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:30:19 INFO 139746470463296] Epoch[31] Batch[0] avg_epoch_loss=2.669206\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:30:19 INFO 139746470463296] #quality_metric: host=algo-1, epoch=31, batch=0 train loss <loss>=2.669206142425537\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:30:29 INFO 139746470463296] Epoch[31] Batch[5] avg_epoch_loss=2.747401\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:30:29 INFO 139746470463296] #quality_metric: host=algo-1, epoch=31, batch=5 train loss <loss>=2.7474008401234946\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:30:29 INFO 139746470463296] Epoch[31] Batch [5]#011Speed: 31.23 samples/sec#011loss=2.747401\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:30:37 INFO 139746470463296] processed a total of 639 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"StartTime\": 1718137805.9253705, \"EndTime\": 1718137837.5727148, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 31646.84748649597, \"count\": 1, \"min\": 31646.84748649597, \"max\": 31646.84748649597}}}\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:30:37 INFO 139746470463296] #throughput_metric: host=algo-1, train throughput=20.191492925617737 records/second\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:30:37 INFO 139746470463296] #progress_metric: host=algo-1, completed 8.0 % of epochs\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:30:37 INFO 139746470463296] #quality_metric: host=algo-1, epoch=31, train loss <loss>=2.770792007446289\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:30:37 INFO 139746470463296] loss did not improve\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:30:50 INFO 139746470463296] Epoch[32] Batch[0] avg_epoch_loss=2.750161\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:30:50 INFO 139746470463296] #quality_metric: host=algo-1, epoch=32, batch=0 train loss <loss>=2.7501614093780518\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:31:00 INFO 139746470463296] Epoch[32] Batch[5] avg_epoch_loss=2.754784\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:31:00 INFO 139746470463296] #quality_metric: host=algo-1, epoch=32, batch=5 train loss <loss>=2.7547840674718223\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:31:00 INFO 139746470463296] Epoch[32] Batch [5]#011Speed: 31.21 samples/sec#011loss=2.754784\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:31:09 INFO 139746470463296] processed a total of 635 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"StartTime\": 1718137837.5728207, \"EndTime\": 1718137869.8537948, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 32280.3795337677, \"count\": 1, \"min\": 32280.3795337677, \"max\": 32280.3795337677}}}\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:31:09 INFO 139746470463296] #throughput_metric: host=algo-1, train throughput=19.67133185875941 records/second\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:31:09 INFO 139746470463296] #progress_metric: host=algo-1, completed 8.25 % of epochs\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:31:09 INFO 139746470463296] #quality_metric: host=algo-1, epoch=32, train loss <loss>=2.755287933349609\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:31:09 INFO 139746470463296] loss did not improve\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:31:22 INFO 139746470463296] Epoch[33] Batch[0] avg_epoch_loss=2.814565\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:31:22 INFO 139746470463296] #quality_metric: host=algo-1, epoch=33, batch=0 train loss <loss>=2.8145649433135986\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:31:32 INFO 139746470463296] Epoch[33] Batch[5] avg_epoch_loss=2.684808\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:31:32 INFO 139746470463296] #quality_metric: host=algo-1, epoch=33, batch=5 train loss <loss>=2.6848078966140747\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:31:32 INFO 139746470463296] Epoch[33] Batch [5]#011Speed: 31.26 samples/sec#011loss=2.684808\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[06/11/2024 20:31:43 INFO 139746470463296] Epoch[33] Batch[10] avg_epoch_loss=2.663760\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:31:43 INFO 139746470463296] #quality_metric: host=algo-1, epoch=33, batch=10 train loss <loss>=2.6385024547576905\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:31:43 INFO 139746470463296] Epoch[33] Batch [10]#011Speed: 28.86 samples/sec#011loss=2.638502\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:31:43 INFO 139746470463296] processed a total of 677 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"StartTime\": 1718137869.8538618, \"EndTime\": 1718137903.6050038, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 33750.55003166199, \"count\": 1, \"min\": 33750.55003166199, \"max\": 33750.55003166199}}}\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:31:43 INFO 139746470463296] #throughput_metric: host=algo-1, train throughput=20.058871848808327 records/second\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:31:43 INFO 139746470463296] #progress_metric: host=algo-1, completed 8.5 % of epochs\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:31:43 INFO 139746470463296] #quality_metric: host=algo-1, epoch=33, train loss <loss>=2.6637599684975366\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:31:43 INFO 139746470463296] best epoch loss so far\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:31:43 INFO 139746470463296] Saved checkpoint to \"/opt/ml/model/state_9df182e7-6c18-44ea-a125-c81576baa9b0-0000.params\"\u001b[0m\n",
      "\u001b[32m#metrics {\"StartTime\": 1718137903.6050727, \"EndTime\": 1718137903.7921, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 186.71131134033203, \"count\": 1, \"min\": 186.71131134033203, \"max\": 186.71131134033203}}}\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:31:56 INFO 139746470463296] Epoch[34] Batch[0] avg_epoch_loss=2.607946\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:31:56 INFO 139746470463296] #quality_metric: host=algo-1, epoch=34, batch=0 train loss <loss>=2.6079461574554443\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:32:07 INFO 139746470463296] Epoch[34] Batch[5] avg_epoch_loss=2.703102\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:32:07 INFO 139746470463296] #quality_metric: host=algo-1, epoch=34, batch=5 train loss <loss>=2.7031015157699585\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:32:07 INFO 139746470463296] Epoch[34] Batch [5]#011Speed: 29.81 samples/sec#011loss=2.703102\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:32:18 INFO 139746470463296] Epoch[34] Batch[10] avg_epoch_loss=2.730247\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:32:18 INFO 139746470463296] #quality_metric: host=algo-1, epoch=34, batch=10 train loss <loss>=2.7628221035003664\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:32:18 INFO 139746470463296] Epoch[34] Batch [10]#011Speed: 30.41 samples/sec#011loss=2.762822\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:32:18 INFO 139746470463296] processed a total of 646 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"StartTime\": 1718137903.7921696, \"EndTime\": 1718137938.201789, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 34409.560203552246, \"count\": 1, \"min\": 34409.560203552246, \"max\": 34409.560203552246}}}\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:32:18 INFO 139746470463296] #throughput_metric: host=algo-1, train throughput=18.773779296584383 records/second\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:32:18 INFO 139746470463296] #progress_metric: host=algo-1, completed 8.75 % of epochs\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:32:18 INFO 139746470463296] #quality_metric: host=algo-1, epoch=34, train loss <loss>=2.730247237465598\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:32:18 INFO 139746470463296] loss did not improve\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:32:31 INFO 139746470463296] Epoch[35] Batch[0] avg_epoch_loss=2.741299\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:32:31 INFO 139746470463296] #quality_metric: host=algo-1, epoch=35, batch=0 train loss <loss>=2.7412991523742676\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:32:41 INFO 139746470463296] Epoch[35] Batch[5] avg_epoch_loss=2.713693\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:32:41 INFO 139746470463296] #quality_metric: host=algo-1, epoch=35, batch=5 train loss <loss>=2.7136934200922647\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:32:41 INFO 139746470463296] Epoch[35] Batch [5]#011Speed: 31.31 samples/sec#011loss=2.713693\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:32:49 INFO 139746470463296] processed a total of 622 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"StartTime\": 1718137938.2018929, \"EndTime\": 1718137969.7768579, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 31574.54824447632, \"count\": 1, \"min\": 31574.54824447632, \"max\": 31574.54824447632}}}\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:32:49 INFO 139746470463296] #throughput_metric: host=algo-1, train throughput=19.69934158762587 records/second\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:32:49 INFO 139746470463296] #progress_metric: host=algo-1, completed 9.0 % of epochs\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:32:49 INFO 139746470463296] #quality_metric: host=algo-1, epoch=35, train loss <loss>=2.7478986501693727\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:32:49 INFO 139746470463296] loss did not improve\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:33:02 INFO 139746470463296] Epoch[36] Batch[0] avg_epoch_loss=2.756322\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:33:02 INFO 139746470463296] #quality_metric: host=algo-1, epoch=36, batch=0 train loss <loss>=2.7563223838806152\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:33:12 INFO 139746470463296] Epoch[36] Batch[5] avg_epoch_loss=2.765592\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:33:12 INFO 139746470463296] #quality_metric: host=algo-1, epoch=36, batch=5 train loss <loss>=2.7655922174453735\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:33:12 INFO 139746470463296] Epoch[36] Batch [5]#011Speed: 30.52 samples/sec#011loss=2.765592\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:33:23 INFO 139746470463296] Epoch[36] Batch[10] avg_epoch_loss=2.744864\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:33:23 INFO 139746470463296] #quality_metric: host=algo-1, epoch=36, batch=10 train loss <loss>=2.719990348815918\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:33:23 INFO 139746470463296] Epoch[36] Batch [10]#011Speed: 29.56 samples/sec#011loss=2.719990\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:33:23 INFO 139746470463296] processed a total of 672 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"StartTime\": 1718137969.7769365, \"EndTime\": 1718138003.784265, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 34006.94704055786, \"count\": 1, \"min\": 34006.94704055786, \"max\": 34006.94704055786}}}\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:33:23 INFO 139746470463296] #throughput_metric: host=algo-1, train throughput=19.76061148820026 records/second\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:33:23 INFO 139746470463296] #progress_metric: host=algo-1, completed 9.25 % of epochs\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:33:23 INFO 139746470463296] #quality_metric: host=algo-1, epoch=36, train loss <loss>=2.7448640953410757\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:33:23 INFO 139746470463296] loss did not improve\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:33:36 INFO 139746470463296] Epoch[37] Batch[0] avg_epoch_loss=2.814822\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:33:36 INFO 139746470463296] #quality_metric: host=algo-1, epoch=37, batch=0 train loss <loss>=2.81482195854187\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:33:46 INFO 139746470463296] Epoch[37] Batch[5] avg_epoch_loss=2.764417\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:33:46 INFO 139746470463296] #quality_metric: host=algo-1, epoch=37, batch=5 train loss <loss>=2.7644166549046836\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:33:46 INFO 139746470463296] Epoch[37] Batch [5]#011Speed: 32.12 samples/sec#011loss=2.764417\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:33:56 INFO 139746470463296] Epoch[37] Batch[10] avg_epoch_loss=2.705778\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:33:56 INFO 139746470463296] #quality_metric: host=algo-1, epoch=37, batch=10 train loss <loss>=2.6354105472564697\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:33:56 INFO 139746470463296] Epoch[37] Batch [10]#011Speed: 31.11 samples/sec#011loss=2.635411\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:33:56 INFO 139746470463296] processed a total of 650 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"StartTime\": 1718138003.7843306, \"EndTime\": 1718138036.8138728, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 33029.096841812134, \"count\": 1, \"min\": 33029.096841812134, \"max\": 33029.096841812134}}}\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:33:56 INFO 139746470463296] #throughput_metric: host=algo-1, train throughput=19.679557219895095 records/second\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:33:56 INFO 139746470463296] #progress_metric: host=algo-1, completed 9.5 % of epochs\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:33:56 INFO 139746470463296] #quality_metric: host=algo-1, epoch=37, train loss <loss>=2.7057775150645864\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:33:56 INFO 139746470463296] loss did not improve\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[06/11/2024 20:34:09 INFO 139746470463296] Epoch[38] Batch[0] avg_epoch_loss=2.686535\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:34:09 INFO 139746470463296] #quality_metric: host=algo-1, epoch=38, batch=0 train loss <loss>=2.6865346431732178\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:34:19 INFO 139746470463296] Epoch[38] Batch[5] avg_epoch_loss=2.707425\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:34:19 INFO 139746470463296] #quality_metric: host=algo-1, epoch=38, batch=5 train loss <loss>=2.707425316174825\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:34:19 INFO 139746470463296] Epoch[38] Batch [5]#011Speed: 32.04 samples/sec#011loss=2.707425\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:34:28 INFO 139746470463296] processed a total of 627 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"StartTime\": 1718138036.8139436, \"EndTime\": 1718138068.0113568, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 31196.917057037354, \"count\": 1, \"min\": 31196.917057037354, \"max\": 31196.917057037354}}}\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:34:28 INFO 139746470463296] #throughput_metric: host=algo-1, train throughput=20.09806068663317 records/second\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:34:28 INFO 139746470463296] #progress_metric: host=algo-1, completed 9.75 % of epochs\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:34:28 INFO 139746470463296] #quality_metric: host=algo-1, epoch=38, train loss <loss>=2.723537540435791\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:34:28 INFO 139746470463296] loss did not improve\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:34:40 INFO 139746470463296] Epoch[39] Batch[0] avg_epoch_loss=2.641359\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:34:40 INFO 139746470463296] #quality_metric: host=algo-1, epoch=39, batch=0 train loss <loss>=2.6413590908050537\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:34:50 INFO 139746470463296] Epoch[39] Batch[5] avg_epoch_loss=2.685287\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:34:50 INFO 139746470463296] #quality_metric: host=algo-1, epoch=39, batch=5 train loss <loss>=2.685287435849508\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:34:50 INFO 139746470463296] Epoch[39] Batch [5]#011Speed: 31.46 samples/sec#011loss=2.685287\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:35:01 INFO 139746470463296] Epoch[39] Batch[10] avg_epoch_loss=2.695997\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:35:01 INFO 139746470463296] #quality_metric: host=algo-1, epoch=39, batch=10 train loss <loss>=2.7088475227355957\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:35:01 INFO 139746470463296] Epoch[39] Batch [10]#011Speed: 28.99 samples/sec#011loss=2.708848\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:35:01 INFO 139746470463296] processed a total of 672 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"StartTime\": 1718138068.0114343, \"EndTime\": 1718138101.5919168, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 33579.9663066864, \"count\": 1, \"min\": 33579.9663066864, \"max\": 33579.9663066864}}}\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:35:01 INFO 139746470463296] #throughput_metric: host=algo-1, train throughput=20.011871971977367 records/second\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:35:01 INFO 139746470463296] #progress_metric: host=algo-1, completed 10.0 % of epochs\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:35:01 INFO 139746470463296] #quality_metric: host=algo-1, epoch=39, train loss <loss>=2.695996566252275\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:35:01 INFO 139746470463296] loss did not improve\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:35:15 INFO 139746470463296] Epoch[40] Batch[0] avg_epoch_loss=2.654402\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:35:15 INFO 139746470463296] #quality_metric: host=algo-1, epoch=40, batch=0 train loss <loss>=2.654402256011963\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:35:25 INFO 139746470463296] Epoch[40] Batch[5] avg_epoch_loss=2.669370\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:35:25 INFO 139746470463296] #quality_metric: host=algo-1, epoch=40, batch=5 train loss <loss>=2.6693703333536782\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:35:25 INFO 139746470463296] Epoch[40] Batch [5]#011Speed: 30.93 samples/sec#011loss=2.669370\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:35:34 INFO 139746470463296] processed a total of 602 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"StartTime\": 1718138101.5919826, \"EndTime\": 1718138134.1098762, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 32517.296314239502, \"count\": 1, \"min\": 32517.296314239502, \"max\": 32517.296314239502}}}\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:35:34 INFO 139746470463296] #throughput_metric: host=algo-1, train throughput=18.513129544915508 records/second\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:35:34 INFO 139746470463296] #progress_metric: host=algo-1, completed 10.25 % of epochs\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:35:34 INFO 139746470463296] #quality_metric: host=algo-1, epoch=40, train loss <loss>=2.6775230407714843\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:35:34 INFO 139746470463296] loss did not improve\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:35:47 INFO 139746470463296] Epoch[41] Batch[0] avg_epoch_loss=2.627371\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:35:47 INFO 139746470463296] #quality_metric: host=algo-1, epoch=41, batch=0 train loss <loss>=2.627371072769165\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:35:57 INFO 139746470463296] Epoch[41] Batch[5] avg_epoch_loss=2.708986\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:35:57 INFO 139746470463296] #quality_metric: host=algo-1, epoch=41, batch=5 train loss <loss>=2.7089856465657554\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:35:57 INFO 139746470463296] Epoch[41] Batch [5]#011Speed: 30.59 samples/sec#011loss=2.708986\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:36:09 INFO 139746470463296] Epoch[41] Batch[10] avg_epoch_loss=2.775090\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:36:09 INFO 139746470463296] #quality_metric: host=algo-1, epoch=41, batch=10 train loss <loss>=2.8544161319732666\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:36:09 INFO 139746470463296] Epoch[41] Batch [10]#011Speed: 28.12 samples/sec#011loss=2.854416\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:36:09 INFO 139746470463296] processed a total of 650 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"StartTime\": 1718138134.1099958, \"EndTime\": 1718138169.1902437, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 35079.795837402344, \"count\": 1, \"min\": 35079.795837402344, \"max\": 35079.795837402344}}}\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:36:09 INFO 139746470463296] #throughput_metric: host=algo-1, train throughput=18.529130335932756 records/second\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:36:09 INFO 139746470463296] #progress_metric: host=algo-1, completed 10.5 % of epochs\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:36:09 INFO 139746470463296] #quality_metric: host=algo-1, epoch=41, train loss <loss>=2.7750904126600786\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:36:09 INFO 139746470463296] loss did not improve\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:36:22 INFO 139746470463296] Epoch[42] Batch[0] avg_epoch_loss=2.722492\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:36:22 INFO 139746470463296] #quality_metric: host=algo-1, epoch=42, batch=0 train loss <loss>=2.7224924564361572\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:36:33 INFO 139746470463296] Epoch[42] Batch[5] avg_epoch_loss=2.708058\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:36:33 INFO 139746470463296] #quality_metric: host=algo-1, epoch=42, batch=5 train loss <loss>=2.70805823802948\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:36:33 INFO 139746470463296] Epoch[42] Batch [5]#011Speed: 30.82 samples/sec#011loss=2.708058\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:36:43 INFO 139746470463296] Epoch[42] Batch[10] avg_epoch_loss=2.751745\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:36:43 INFO 139746470463296] #quality_metric: host=algo-1, epoch=42, batch=10 train loss <loss>=2.804169511795044\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:36:43 INFO 139746470463296] Epoch[42] Batch [10]#011Speed: 30.07 samples/sec#011loss=2.804170\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:36:43 INFO 139746470463296] processed a total of 642 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"StartTime\": 1718138169.1903145, \"EndTime\": 1718138203.9404335, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 34749.738693237305, \"count\": 1, \"min\": 34749.738693237305, \"max\": 34749.738693237305}}}\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:36:43 INFO 139746470463296] #throughput_metric: host=algo-1, train throughput=18.474910013701074 records/second\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:36:43 INFO 139746470463296] #progress_metric: host=algo-1, completed 10.75 % of epochs\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:36:43 INFO 139746470463296] #quality_metric: host=algo-1, epoch=42, train loss <loss>=2.751745180650191\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:36:43 INFO 139746470463296] loss did not improve\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:36:57 INFO 139746470463296] Epoch[43] Batch[0] avg_epoch_loss=2.702107\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:36:57 INFO 139746470463296] #quality_metric: host=algo-1, epoch=43, batch=0 train loss <loss>=2.7021074295043945\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[06/11/2024 20:37:08 INFO 139746470463296] Epoch[43] Batch[5] avg_epoch_loss=2.708628\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:37:08 INFO 139746470463296] #quality_metric: host=algo-1, epoch=43, batch=5 train loss <loss>=2.7086277405420938\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:37:08 INFO 139746470463296] Epoch[43] Batch [5]#011Speed: 29.56 samples/sec#011loss=2.708628\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:37:18 INFO 139746470463296] Epoch[43] Batch[10] avg_epoch_loss=2.747630\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:37:18 INFO 139746470463296] #quality_metric: host=algo-1, epoch=43, batch=10 train loss <loss>=2.794431734085083\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:37:18 INFO 139746470463296] Epoch[43] Batch [10]#011Speed: 29.40 samples/sec#011loss=2.794432\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:37:18 INFO 139746470463296] processed a total of 656 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"StartTime\": 1718138203.940493, \"EndTime\": 1718138238.904068, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 34962.92996406555, \"count\": 1, \"min\": 34962.92996406555, \"max\": 34962.92996406555}}}\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:37:18 INFO 139746470463296] #throughput_metric: host=algo-1, train throughput=18.762670334102033 records/second\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:37:18 INFO 139746470463296] #progress_metric: host=algo-1, completed 11.0 % of epochs\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:37:18 INFO 139746470463296] #quality_metric: host=algo-1, epoch=43, train loss <loss>=2.747629555788907\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:37:18 INFO 139746470463296] loss did not improve\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:37:18 INFO 139746470463296] Loading parameters from best epoch (33)\u001b[0m\n",
      "\u001b[32m#metrics {\"StartTime\": 1718138238.9041467, \"EndTime\": 1718138238.984319, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.deserialize.time\": {\"sum\": 79.69474792480469, \"count\": 1, \"min\": 79.69474792480469, \"max\": 79.69474792480469}}}\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:37:18 INFO 139746470463296] stopping training now\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:37:18 INFO 139746470463296] #progress_metric: host=algo-1, completed 100 % of epochs\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:37:18 INFO 139746470463296] Final loss: 2.6637599684975366 (occurred at epoch 33)\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:37:18 INFO 139746470463296] #quality_metric: host=algo-1, train final_loss <loss>=2.6637599684975366\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:37:18 WARNING 139746470463296] You are using large values for `context_length` and/or `prediction_length`. The following step may take some time. If the step crashes, use an instance with more memory or reduce these two parameters.\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:37:18 INFO 139746470463296] Worker algo-1 finished training.\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:37:18 WARNING 139746470463296] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:37:18 INFO 139746470463296] All workers finished. Serializing model for prediction.\u001b[0m\n",
      "\u001b[32m#metrics {\"StartTime\": 1718138238.9843931, \"EndTime\": 1718138252.3886197, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"get_graph.time\": {\"sum\": 13403.496980667114, \"count\": 1, \"min\": 13403.496980667114, \"max\": 13403.496980667114}}}\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:37:33 INFO 139746470463296] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[32m#metrics {\"StartTime\": 1718138252.3886957, \"EndTime\": 1718138253.2452488, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"finalize.time\": {\"sum\": 14260.164022445679, \"count\": 1, \"min\": 14260.164022445679, \"max\": 14260.164022445679}}}\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:37:33 INFO 139746470463296] Serializing to /opt/ml/model/model_algo-1\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:37:33 INFO 139746470463296] Saved checkpoint to \"/opt/ml/model/model_algo-1-0000.params\"\u001b[0m\n",
      "\u001b[32m#metrics {\"StartTime\": 1718138253.2453232, \"EndTime\": 1718138253.3742242, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"model.serialize.time\": {\"sum\": 128.86333465576172, \"count\": 1, \"min\": 128.86333465576172, \"max\": 128.86333465576172}}}\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:37:33 INFO 139746470463296] Successfully serialized the model for prediction.\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:37:33 INFO 139746470463296] #memory_usage::<batchbuffer> = 385.8251953125 mb\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:37:33 INFO 139746470463296] Evaluating model accuracy on testset using 100 samples\u001b[0m\n",
      "\u001b[32m#metrics {\"StartTime\": 1718138253.3742905, \"EndTime\": 1718138253.379933, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"model.bind.time\": {\"sum\": 0.03981590270996094, \"count\": 1, \"min\": 0.03981590270996094, \"max\": 0.03981590270996094}}}\u001b[0m\n",
      "\u001b[32m#metrics {\"StartTime\": 1718138253.3799927, \"EndTime\": 1718138259.9308295, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"model.score.time\": {\"sum\": 6550.938606262207, \"count\": 1, \"min\": 6550.938606262207, \"max\": 6550.938606262207}}}\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:37:39 INFO 139746470463296] #test_score (algo-1, RMSE): 56.145324679922716\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:37:39 INFO 139746470463296] #test_score (algo-1, mean_absolute_QuantileLoss): 12314.18079866237\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:37:39 INFO 139746470463296] #test_score (algo-1, mean_wQuantileLoss): 0.1587206227915855\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:37:39 INFO 139746470463296] #test_score (algo-1, wQuantileLoss[0.1]): 0.09748757715709776\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:37:39 INFO 139746470463296] #test_score (algo-1, wQuantileLoss[0.2]): 0.1461026375534412\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:37:39 INFO 139746470463296] #test_score (algo-1, wQuantileLoss[0.3]): 0.17534532511749323\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:37:39 INFO 139746470463296] #test_score (algo-1, wQuantileLoss[0.4]): 0.18836972140179858\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:37:39 INFO 139746470463296] #test_score (algo-1, wQuantileLoss[0.5]): 0.1916001609160307\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:37:39 INFO 139746470463296] #test_score (algo-1, wQuantileLoss[0.6]): 0.18975133997714502\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:37:39 INFO 139746470463296] #test_score (algo-1, wQuantileLoss[0.7]): 0.18198852353451006\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:37:39 INFO 139746470463296] #test_score (algo-1, wQuantileLoss[0.8]): 0.15392178334349527\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:37:39 INFO 139746470463296] #test_score (algo-1, wQuantileLoss[0.9]): 0.10391853612325785\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:37:39 INFO 139746470463296] #quality_metric: host=algo-1, test RMSE <loss>=56.145324679922716\u001b[0m\n",
      "\u001b[32m[06/11/2024 20:37:39 INFO 139746470463296] #quality_metric: host=algo-1, test mean_wQuantileLoss <loss>=0.1587206227915855\u001b[0m\n",
      "\u001b[32m#metrics {\"StartTime\": 1718138259.9308991, \"EndTime\": 1718138260.2366242, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"setuptime\": {\"sum\": 4.476070404052734, \"count\": 1, \"min\": 4.476070404052734, \"max\": 4.476070404052734}, \"totaltime\": {\"sum\": 1505843.7056541443, \"count\": 1, \"min\": 1505843.7056541443, \"max\": 1505843.7056541443}}}\u001b[0m\n",
      "\n",
      "2024-06-11 20:37:54 Uploading - Uploading generated training model\n",
      "2024-06-11 20:37:54 Completed - Training job completed\n",
      "Training seconds: 1750\n",
      "Billable seconds: 765\n",
      "Managed Spot Training savings: 56.3%\n"
     ]
    }
   ],
   "source": [
    "# fitting the model by passing channel details\n",
    "\n",
    "estimator.fit(inputs=data_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fec84859",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = estimator.latest_training_job.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a9e7aee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job name: deepar-biketrain-with-dynamic-feat-2024-06-11-19-51-55-174\n"
     ]
    }
   ],
   "source": [
    "print('job name: {}'.format(job_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1d8edc08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: deepar-biketrain-with-dynamic-feat-2024-06-11-19-51-55-174\n",
      "INFO:sagemaker:Creating endpoint-config with name deepar-biketrain-with-dynamic-feat-2024-06-11-19-51-55-174\n",
      "INFO:sagemaker:Creating endpoint with name deepar-biketrain-with-dynamic-feat-2024-06-11-19-51-55-174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------!"
     ]
    }
   ],
   "source": [
    "# Create an endport for real-time predictions\n",
    "\n",
    "endpoint_name = sess.endpoint_from_job(\n",
    "    job_name = job_name,\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    image_uri=container,\n",
    "    role=role\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1e30b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p310",
   "language": "python",
   "name": "conda_tensorflow2_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
